{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Lab01: Portugese Bank Marketing Data'\n",
    "subtitle: \"MSDS 7331\"\n",
    "authors: \"Anthony Burton-Cordova, Will Jones, Nick Sager\"\n",
    "date: September 24, 2023\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For an introduction to the data, business understanding, and explanation of the the dataset, please see [Lab01](Lab01.ipynb), which contains the exploratory data analysis (EDA) from Lab 01. This notebook will focus on the modeling of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "### Reference only - delete before submitting\n",
    "\n",
    "| Category                 | Available | Requirements |\n",
    "|--------------------------|-----------|--------------|\n",
    "| Total Points             | 100       | Total             |\n",
    "| Create Models            | 50        | Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. |\n",
    "| Model Advantages         | 10        | Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail. |\n",
    "| Interpret Feature Importance | 30    | Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important? |\n",
    "| Interpret Support Vectors   | 10     | Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC modelâ€” then analyze the support vectors from the subsampled dataset. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Process Data\n",
    "\n",
    "The following code chunks are explained in more detail in [Lab01](Lab01.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Choose File\n",
    "# RawBank = \"https://raw.githubusercontent.com/NickSager/DS7331_Projects/main/data/bank-additional-full.csv\"\n",
    "RawBank = \"data/bank-additional-full.csv\"\n",
    "\n",
    "# Read the CSV file with a semicolon ; separator\n",
    "bank = pd.read_csv(RawBank, sep=';')\n",
    "\n",
    "# Get info on the dataset\n",
    "# print(bank.info())\n",
    "# bank.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# let's set those values to NaN, so that Pandas understand they are missing\n",
    "df = bank.copy() # make a copy of the dataframe\n",
    "df = df.replace(to_replace = 'unknown', value = np.nan) # replace unknown with NaN (not a number)\n",
    "df = df.replace(to_replace = 999, value = np.nan) # replace 999 with NaN (not a number)\n",
    "df = df.replace(to_replace = 'nonexistent', value = np.nan) # replace nonexistent with NaN (not a number)\n",
    "\n",
    "# print (df.info())\n",
    "# df.describe() # scroll over to see the values\n",
    "\n",
    "# From course material \"01. Pandas.ibynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     41188\n",
       "unique        3\n",
       "top       Young\n",
       "freq      23768\n",
       "Name: age_range, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change NA Categoricals to 'unknown'\n",
    "df['job'] = df['job'].fillna('unknown')\n",
    "df['marital'] = df['marital'].fillna('unknown')\n",
    "df['education'] = df['education'].fillna('unknown')\n",
    "\n",
    "# Change NA Credit history values to 'no'\n",
    "df['default'] = df['default'].fillna('no')\n",
    "df['housing'] = df['housing'].fillna('no')\n",
    "df['loan'] = df['loan'].fillna('no')\n",
    "\n",
    "# Change NA Previous Outcome to 'not contacted'\n",
    "df['poutcome'] = df['poutcome'].fillna('not contacted')\n",
    "\n",
    "# Change NA pdays to the mean\n",
    "df['pdays'] = df['pdays'].fillna(df['pdays'].mean())\n",
    "\n",
    "# Change NA Duration to '999'\n",
    "df['duration'] = df['duration'].fillna(999)\n",
    "\n",
    "# let's break up the age variable\n",
    "df['age_range'] = pd.cut(df.age,[0,40,60,1e6],3,labels=['Young','Middle-Age','Old']) # this creates a new variable\n",
    "df.age_range.describe()\n",
    "\n",
    "# print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 53 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   age                            41188 non-null  int64  \n",
      " 1   duration                       41188 non-null  float64\n",
      " 2   campaign                       41188 non-null  int64  \n",
      " 3   pdays                          41188 non-null  float64\n",
      " 4   previous                       41188 non-null  int64  \n",
      " 5   emp.var.rate                   41188 non-null  float64\n",
      " 6   cons.price.idx                 41188 non-null  float64\n",
      " 7   cons.conf.idx                  41188 non-null  float64\n",
      " 8   euribor3m                      41188 non-null  float64\n",
      " 9   nr.employed                    41188 non-null  float64\n",
      " 10  y                              41188 non-null  object \n",
      " 11  job_blue-collar                41188 non-null  uint8  \n",
      " 12  job_entrepreneur               41188 non-null  uint8  \n",
      " 13  job_housemaid                  41188 non-null  uint8  \n",
      " 14  job_management                 41188 non-null  uint8  \n",
      " 15  job_retired                    41188 non-null  uint8  \n",
      " 16  job_self-employed              41188 non-null  uint8  \n",
      " 17  job_services                   41188 non-null  uint8  \n",
      " 18  job_student                    41188 non-null  uint8  \n",
      " 19  job_technician                 41188 non-null  uint8  \n",
      " 20  job_unemployed                 41188 non-null  uint8  \n",
      " 21  job_unknown                    41188 non-null  uint8  \n",
      " 22  marital_married                41188 non-null  uint8  \n",
      " 23  marital_single                 41188 non-null  uint8  \n",
      " 24  marital_unknown                41188 non-null  uint8  \n",
      " 25  education_basic.6y             41188 non-null  uint8  \n",
      " 26  education_basic.9y             41188 non-null  uint8  \n",
      " 27  education_high.school          41188 non-null  uint8  \n",
      " 28  education_illiterate           41188 non-null  uint8  \n",
      " 29  education_professional.course  41188 non-null  uint8  \n",
      " 30  education_university.degree    41188 non-null  uint8  \n",
      " 31  education_unknown              41188 non-null  uint8  \n",
      " 32  default_yes                    41188 non-null  uint8  \n",
      " 33  housing_yes                    41188 non-null  uint8  \n",
      " 34  loan_yes                       41188 non-null  uint8  \n",
      " 35  contact_telephone              41188 non-null  uint8  \n",
      " 36  month_aug                      41188 non-null  uint8  \n",
      " 37  month_dec                      41188 non-null  uint8  \n",
      " 38  month_jul                      41188 non-null  uint8  \n",
      " 39  month_jun                      41188 non-null  uint8  \n",
      " 40  month_mar                      41188 non-null  uint8  \n",
      " 41  month_may                      41188 non-null  uint8  \n",
      " 42  month_nov                      41188 non-null  uint8  \n",
      " 43  month_oct                      41188 non-null  uint8  \n",
      " 44  month_sep                      41188 non-null  uint8  \n",
      " 45  day_of_week_mon                41188 non-null  uint8  \n",
      " 46  day_of_week_thu                41188 non-null  uint8  \n",
      " 47  day_of_week_tue                41188 non-null  uint8  \n",
      " 48  day_of_week_wed                41188 non-null  uint8  \n",
      " 49  poutcome_not contacted         41188 non-null  uint8  \n",
      " 50  poutcome_success               41188 non-null  uint8  \n",
      " 51  age_range_Middle-Age           41188 non-null  uint8  \n",
      " 52  age_range_Old                  41188 non-null  uint8  \n",
      "dtypes: float64(7), int64(3), object(1), uint8(42)\n",
      "memory usage: 5.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Convert all features to numeric using dummy variables\n",
    "df = pd.get_dummies(df, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome', 'age_range'], drop_first=True)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models\n",
    "\n",
    "In this section, we will create Logistic Regression and Support Vector Machine (SVM) models to classify whether a customer will subscribe to a term deposit. We will split the data into training and validation sets using an 80/20 split. ROC - AUC will be used to evaluate the models.\n",
    "\n",
    "The code in this section is adopted from the course material in the notebook '04. Logits and SVM.ipynb'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will change the data into a format that Scikit-Learn can use. We will also split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Consider deleting duration for practicality\n",
    "# if 'duration' in df: del df['duration']\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "X = df.drop(columns=['y']).values\n",
    "y = df['y'].values\n",
    "    \n",
    "# Split into training and test sets\n",
    "X_train_holdout, X_test_holdout, y_train_holdout, y_test_holdout = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=137, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'holdout' splits will be used to evaluate the final model on unseen data. We will additionally use cross-validation to tune the hyperparameters of the models.\n",
    "\n",
    "Next we will define the parts of the pipeline that will be used to transform the data and fit the models. We will use a StandardScaler to scale the data, and a PCA to reduce the dimensionality of the data. We will use a LogisticRegression model and a SVC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold: 1 is 0.9090\n",
      "Accuracy for fold: 2 is 0.9123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold: 3 is 0.9073\n",
      "Accuracy for fold: 4 is 0.9149\n",
      "Accuracy for fold: 5 is 0.9109\n",
      "\n",
      "Overall accuracy: 0.9109 (+/- 0.0026)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracies = []\n",
    "\n",
    "# Define the cross validation method on training holdout\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, val_index in skf.split(X_train_holdout, y_train_holdout):\n",
    "    # Splitting the data\n",
    "    X_train, X_val = X_train_holdout[train_index], X_train_holdout[val_index]\n",
    "    y_train, y_val = y_train_holdout[train_index], y_train_holdout[val_index]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = pipe.predict(X_val)\n",
    "    \n",
    "    # Calculate accuracy or any other metric\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    # Optionally, print the accuracy for each fold\n",
    "    print(f\"Accuracy for fold: {len(accuracies)} is {acc:.4f}\")\n",
    "\n",
    "# Calculate mean and std deviation of the accuracies\n",
    "mean_acc = np.mean(accuracies)\n",
    "std_acc = np.std(accuracies)\n",
    "\n",
    "print(f\"\\nOverall accuracy: {mean_acc:.4f} (+/- {std_acc:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will evaluate the final model on the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Performance on Holdout Set: 0.9090\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(X_train_holdout, y_train_holdout)\n",
    "final_performance = pipe.score(X_test_holdout, y_test_holdout)\n",
    "print(f\"Final Model Performance on Holdout Set: {final_performance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Feature Importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Support Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "@article{Moro2014ADA,\n",
    "  title={A data-driven approach to predict the success of bank telemarketing},\n",
    "  author={S{\\'e}rgio Moro and P. Cortez and Paulo Rita},\n",
    "  journal={Decis. Support Syst.},\n",
    "  year={2014},\n",
    "  volume={62},\n",
    "  pages={22-31},\n",
    "  url={https://api.semanticscholar.org/CorpusID:14181100}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
