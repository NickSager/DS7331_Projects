{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Lab01: Portugese Bank Marketing Data'\n",
    "subtitle: \"MSDS 7331\"\n",
    "authors: \"Anthony Burton-Cordova, Will Jones, Nick Sager\"\n",
    "date: September 24, 2023\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For an introduction to the data, business understanding, and explanation of the the dataset, please see [Lab01](Lab01.ipynb), which contains the exploratory data analysis (EDA) from Lab 01. This notebook will focus on the modeling of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "### Reference only - delete before submitting\n",
    "\n",
    "| Category                 | Available | Requirements |\n",
    "|--------------------------|-----------|--------------|\n",
    "| Total Points             | 100       | Total             |\n",
    "| Create Models            | 50        | Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. |\n",
    "| Model Advantages         | 10        | Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail. |\n",
    "| Interpret Feature Importance | 30    | Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important? |\n",
    "| Interpret Support Vectors   | 10     | Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC modelâ€” then analyze the support vectors from the subsampled dataset. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Process Data\n",
    "\n",
    "The following code chunks are explained in more detail in [Lab01](Lab01.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Choose File\n",
    "RawBank = \"https://raw.githubusercontent.com/NickSager/DS7331_Projects/main/data/bank-additional-full.csv\"\n",
    "# RawBank = \"data/bank-additional-full.csv\"\n",
    "\n",
    "# Read the CSV file with a semicolon ; separator\n",
    "bank = pd.read_csv(RawBank, sep=';')\n",
    "\n",
    "# Get info on the dataset\n",
    "# print(bank.info())\n",
    "# bank.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# let's set those values to NaN, so that Pandas understand they are missing\n",
    "df = bank.copy() # make a copy of the dataframe\n",
    "df = df.replace(to_replace = 'unknown', value = np.nan) # replace unknown with NaN (not a number)\n",
    "df = df.replace(to_replace = 999, value = np.nan) # replace 999 with NaN (not a number)\n",
    "df = df.replace(to_replace = 'nonexistent', value = np.nan) # replace nonexistent with NaN (not a number)\n",
    "\n",
    "# print (df.info())\n",
    "# df.describe() # scroll over to see the values\n",
    "\n",
    "# From course material \"01. Pandas.ibynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change NA Categoricals to 'unknown'\n",
    "df['job'] = df['job'].fillna('unknown')\n",
    "df['marital'] = df['marital'].fillna('unknown')\n",
    "df['education'] = df['education'].fillna('unknown')\n",
    "\n",
    "# Change NA Credit history values to 'no'\n",
    "df['default'] = df['default'].fillna('no')\n",
    "df['housing'] = df['housing'].fillna('no')\n",
    "df['loan'] = df['loan'].fillna('no')\n",
    "\n",
    "# Change NA Previous Outcome to 'not contacted'\n",
    "df['poutcome'] = df['poutcome'].fillna('not contacted')\n",
    "\n",
    "# Change NA pdays to the mean\n",
    "df['pdays'] = df['pdays'].fillna(df['pdays'].mean())\n",
    "\n",
    "# Change NA Duration to '999'\n",
    "df['duration'] = df['duration'].fillna(999)\n",
    "\n",
    "# let's break up the age variable\n",
    "df['age_range'] = pd.cut(df.age,[0,40,60,1e6],3,labels=['Young','Middle-Age','Old']) # this creates a new variable\n",
    "# df.age_range.describe()\n",
    "\n",
    "# print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 53 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   age                            41188 non-null  int64  \n",
      " 1   duration                       41188 non-null  float64\n",
      " 2   campaign                       41188 non-null  int64  \n",
      " 3   pdays                          41188 non-null  float64\n",
      " 4   previous                       41188 non-null  int64  \n",
      " 5   emp.var.rate                   41188 non-null  float64\n",
      " 6   cons.price.idx                 41188 non-null  float64\n",
      " 7   cons.conf.idx                  41188 non-null  float64\n",
      " 8   euribor3m                      41188 non-null  float64\n",
      " 9   nr.employed                    41188 non-null  float64\n",
      " 10  y                              41188 non-null  object \n",
      " 11  job_blue-collar                41188 non-null  bool   \n",
      " 12  job_entrepreneur               41188 non-null  bool   \n",
      " 13  job_housemaid                  41188 non-null  bool   \n",
      " 14  job_management                 41188 non-null  bool   \n",
      " 15  job_retired                    41188 non-null  bool   \n",
      " 16  job_self-employed              41188 non-null  bool   \n",
      " 17  job_services                   41188 non-null  bool   \n",
      " 18  job_student                    41188 non-null  bool   \n",
      " 19  job_technician                 41188 non-null  bool   \n",
      " 20  job_unemployed                 41188 non-null  bool   \n",
      " 21  job_unknown                    41188 non-null  bool   \n",
      " 22  marital_married                41188 non-null  bool   \n",
      " 23  marital_single                 41188 non-null  bool   \n",
      " 24  marital_unknown                41188 non-null  bool   \n",
      " 25  education_basic.6y             41188 non-null  bool   \n",
      " 26  education_basic.9y             41188 non-null  bool   \n",
      " 27  education_high.school          41188 non-null  bool   \n",
      " 28  education_illiterate           41188 non-null  bool   \n",
      " 29  education_professional.course  41188 non-null  bool   \n",
      " 30  education_university.degree    41188 non-null  bool   \n",
      " 31  education_unknown              41188 non-null  bool   \n",
      " 32  default_yes                    41188 non-null  bool   \n",
      " 33  housing_yes                    41188 non-null  bool   \n",
      " 34  loan_yes                       41188 non-null  bool   \n",
      " 35  contact_telephone              41188 non-null  bool   \n",
      " 36  month_aug                      41188 non-null  bool   \n",
      " 37  month_dec                      41188 non-null  bool   \n",
      " 38  month_jul                      41188 non-null  bool   \n",
      " 39  month_jun                      41188 non-null  bool   \n",
      " 40  month_mar                      41188 non-null  bool   \n",
      " 41  month_may                      41188 non-null  bool   \n",
      " 42  month_nov                      41188 non-null  bool   \n",
      " 43  month_oct                      41188 non-null  bool   \n",
      " 44  month_sep                      41188 non-null  bool   \n",
      " 45  day_of_week_mon                41188 non-null  bool   \n",
      " 46  day_of_week_thu                41188 non-null  bool   \n",
      " 47  day_of_week_tue                41188 non-null  bool   \n",
      " 48  day_of_week_wed                41188 non-null  bool   \n",
      " 49  poutcome_not contacted         41188 non-null  bool   \n",
      " 50  poutcome_success               41188 non-null  bool   \n",
      " 51  age_range_Middle-Age           41188 non-null  bool   \n",
      " 52  age_range_Old                  41188 non-null  bool   \n",
      "dtypes: bool(42), float64(7), int64(3), object(1)\n",
      "memory usage: 5.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Convert all features to numeric using dummy variables\n",
    "df = pd.get_dummies(df, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome', 'age_range'], drop_first=True)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models\n",
    "\n",
    "In this section, we will create Logistic Regression and Support Vector Machine (SVM) models to classify whether a customer will subscribe to a term deposit. We will split the data into training and validation sets using an 80/20 split. ROC - AUC will be used to evaluate the models.\n",
    "\n",
    "Due to previous work in Lab01.ipynb it was shown that our response variable, whether a customer will subscribe to a term deposit, was highly unbalanced with only 11% of the variable data being 'yes' so for the metric we will choose ROC-AUC. The common metric accuracy would not be good for this model due to it only being able to assess the correct number of true predictions in the samples from the dataset but fails to take into account the false positives. \n",
    "\n",
    "The Area Under the Curve is great for classification models and understands the relationship between the true positive rate and the false positive rate. It's value is contained in the range of 0.5 and 1 being that if the models auc is 0.5 it predicts no better than a flip of a coin at random or 1 being that it is the perfect classifier. \n",
    "\n",
    "The code in this section is adopted from the course material in the notebook '04. Logits and SVM.ipynb'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will change the data into a format that Scikit-Learn can use. We will also split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Consider deleting duration for practicality\n",
    "if 'duration' in df: del df['duration']\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "X = df.drop(columns=['y']).values\n",
    "y = df['y'].values\n",
    "    \n",
    "# Split into training and test sets\n",
    "X_train_holdout, X_test_holdout, y_train_holdout, y_test_holdout = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=137, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'holdout' splits will be used to evaluate the final model on unseen data. We will additionally use cross-validation to tune the hyperparameters of the models.\n",
    "\n",
    "Next we will define the parts of the pipeline that will be used to transform the data and fit the models. We will use a StandardScaler to scale the data, and experiment with using PCA to reduce the dimensionality of the data. We will use a LogisticRegression model and a SVC model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the model\n",
    "lr_model = LogisticRegression(penalty='l2',\n",
    "                           C=1.0,\n",
    "                           class_weight=None,\n",
    "                           solver='liblinear',\n",
    "                           random_state=137,\n",
    "                           max_iter=150)\n",
    "\n",
    "# Define the pipeline\n",
    "lr_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', lr_model)\n",
    "])\n",
    "\n",
    "# Define the cross validation method on training holdout\n",
    "skf = StratifiedKFold(n_splits=5, random_state=137, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC for fold: 1 is 0.8019\n",
      "ROC-AUC for fold: 2 is 0.7869\n",
      "ROC-AUC for fold: 3 is 0.7796\n",
      "ROC-AUC for fold: 4 is 0.7929\n",
      "ROC-AUC for fold: 5 is 0.8078\n",
      "\n",
      "Overall ROC-AUC: 0.7938 (+/- 0.0101)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "rocauc_lr = []\n",
    "\n",
    "for train_index, val_index in skf.split(X_train_holdout, y_train_holdout):\n",
    "    # Splitting the data\n",
    "    X_train, X_val = X_train_holdout[train_index], X_train_holdout[val_index]\n",
    "    y_train, y_val = y_train_holdout[train_index], y_train_holdout[val_index]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    lr_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    # y_pred = lr_pipe.predict(X_val) # Incorrect\n",
    "    y_pred_prob = lr_pipe.predict_proba(X_val)[:, 1] # Need to use probability estimates here\n",
    "\n",
    "    # Calculate ROC-AUC\n",
    "    ra = roc_auc_score(y_val, y_pred_prob)\n",
    "    rocauc_lr.append(ra)\n",
    "    \n",
    "    # Optionally, print the ROC-AUC for each fold\n",
    "    print(f\"ROC-AUC for fold: {len(rocauc_lr)} is {ra:.4f}\")\n",
    "\n",
    "# Calculate mean and std deviation of the ROC-AUC's\n",
    "mean_ra_lr = np.mean(rocauc_lr)\n",
    "std_ra_lr = np.std(rocauc_lr)\n",
    "\n",
    "print(f\"\\nOverall ROC-AUC: {mean_ra_lr:.4f} (+/- {std_ra_lr:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80186467 0.78693236 0.77961941 0.79288789 0.80776961]\n",
      "\n",
      "Overall ROC-AUC: 0.7938 (+/- 0.0101)\n"
     ]
    }
   ],
   "source": [
    "# Implement the same thing a different way for practice\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rocauc_lr2 = cross_val_score(estimator=lr_pipe,\n",
    "                                X=X_train_holdout,\n",
    "                                y=y_train_holdout,\n",
    "                                cv=skf,\n",
    "                                scoring=\"roc_auc\",\n",
    "                                n_jobs=-1)\n",
    "print(rocauc_lr2)\n",
    "\n",
    "# Calculate mean and std deviation of the accuracies\n",
    "mean_ra_lr2 = np.mean(rocauc_lr2)\n",
    "std_ra_lr2 = np.std(rocauc_lr2)\n",
    "\n",
    "print(f\"\\nOverall ROC-AUC: {mean_ra_lr2:.4f} (+/- {std_ra_lr2:.4f})\")\n",
    "# Same results and faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC of each fold is approximately 79%. This is about 14% lower than when 'duration' is included. By removing it, we have a more practical model with only a small loss in predictive power. This small loss in predictive power is a good trade off to have a model that is useful in the real world.\n",
    "\n",
    "Next, we will evaluate the final model on the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Performance on Holdout Set: 0.7785\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "lr_pipe.fit(X_train_holdout, y_train_holdout)\n",
    "\n",
    "#prediction on the test set\n",
    "final_y_pred_prob = lr_pipe.predict_proba(X_test_holdout)[:,1    ]\n",
    "\n",
    "#calculating the ROC-AUC performance\n",
    "final_performance = roc_auc_score(y_test_holdout, final_y_pred_prob)\n",
    "print(f\"Final Model Performance on Holdout Set: {final_performance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model achieves an accuracy of 77.85% on the holdout set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, we will use a grid search with the cross validation object to tune the hyperparameters of the Logistic Regression model. The parameters in the grid search are a rough estimate, and further tuning could be done on a narrower range once the best parameters are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "240 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 66, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1179, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\antho\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.76378575        nan 0.76378215 0.78807711\n",
      " 0.78807596 0.78841665 0.78807649 0.78807605        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.78866927\n",
      "        nan 0.78864609 0.78924224 0.78924191 0.78927001 0.78923292\n",
      " 0.78914497        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.79115461        nan 0.79013289 0.79078378\n",
      " 0.79078024 0.7907757  0.79032413 0.7897229         nan        nan\n",
      "        nan        nan        nan        nan        nan 0.79069629\n",
      "        nan 0.78985292 0.79066172 0.79065803 0.79065995 0.7904622\n",
      " 0.78981274        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.79059675        nan 0.78982191 0.79059744\n",
      " 0.79060075 0.7905965  0.79047438 0.78982161        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.79059237\n",
      "        nan 0.78982338 0.79058574 0.79058712 0.79058541 0.79047668\n",
      " 0.78982146        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'classifier__C': 0.1, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\n",
      "Best cross-validation score (ROC-AUC):  0.7911546135814282\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "param_grid_logistic = {\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(lr_pipe, param_grid_logistic,\n",
    "                           cv=skf, n_jobs=-1, error_score=np.nan,\n",
    "                           scoring='roc_auc')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score (ROC-AUC): \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters are C=0.1 and penalty='l1' using the 'liblinear' solver. The best accuracy ROC-AUC 79.1%, which is comparable to the baseline model without tuning. Many combinations of parameters wither failed to converge or failed due to mismatch of the solver and penalty, but the grid search can be forced to run by setting `error_score=np.nan`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement our Support Vector machine which will be done in similar fashion as the Logistic Regression in the section above. One model will be made using a pipeline and the other with the cross_val_score function. We will also implement the svm later on using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the model\n",
    "svm_model = SVC(C=1.0,\n",
    "            kernel='rbf',\n",
    "            gamma='scale',\n",
    "            class_weight=None,\n",
    "            random_state=137)\n",
    "\n",
    "# Define the pipeline\n",
    "svm_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', svm_model)\n",
    "])\n",
    "\n",
    "# Define the cross validation method on training holdout\n",
    "skf = StratifiedKFold(n_splits=5, random_state=137, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a pipeline for the SVM model. We will use a StandardScaler to scale the data, and the 'rbf' kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71711941 0.72241794 0.71886281 0.71558283 0.72143967]\n",
      "\n",
      "Overall ROC-AUC: 0.7191 (+/- 0.0026)\n"
     ]
    }
   ],
   "source": [
    "# Implement CV using the shorter method with much faster processing speed\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rocauc_svm = cross_val_score(estimator=svm_pipe,\n",
    "                                X=X_train_holdout,\n",
    "                                y=y_train_holdout,\n",
    "                                scoring = \"roc_auc\",\n",
    "                                cv=skf,\n",
    "                                n_jobs=-1)\n",
    "print(rocauc_svm)\n",
    "\n",
    "# Calculate mean and std deviation of the accuracies\n",
    "mean_ra_svm = np.mean(rocauc_svm)\n",
    "std_ra_svm = np.std(rocauc_svm)\n",
    "\n",
    "print(f\"\\nOverall ROC-AUC: {mean_ra_svm:.4f} (+/- {std_ra_svm:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC of each fold is approximately 71%. This is about 8% lower than the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Performance on Holdout Set: 0.7030\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "svm_pipe.fit(X_train_holdout, y_train_holdout)\n",
    "\n",
    "#prediction on the test set\n",
    "final_y_pred_prob = svm_pipe.decision_function(X_test_holdout)#[:,1] # Using decision_function instead of predict_proba\n",
    "\n",
    "#calculating the ROC-AUC performance\n",
    "final_performance = roc_auc_score(y_test_holdout, final_y_pred_prob)\n",
    "print(f\"Final Model Performance on Holdout Set: {final_performance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model achieves an AUC of 70.3% on the holdout set. This is approximately 8% lower than the Logistic Regression model as predicted by the cross validation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, we will use a grid search with the cross validation object to tune the hyperparameters of the SVM model. The parameters in the grid search are a rough estimate, and further tuning could be done on a narrower range once the best parameters are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this. 51m to run on my machine\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define hyperparameter space\n",
    "# param_grid = {'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "#               'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# # Use GridSearchCV\n",
    "# grid_search = GridSearchCV(svm_pipe, param_grid, cv=skf, n_jobs=-1, scoring='roc_auc')\n",
    "# grid_search.fit(X, y)\n",
    "\n",
    "# print(\"Best parameters: \", grid_search.best_params_)\n",
    "# print(\"Best cross-validation score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search yields an optimal `C` of `1.0`, and `gamma` of `0.01`. The accuracy of the model with these parameters is 89.9%, which is equivalent to the model using the default parameters. The grid search was not evaluated using ROC-AUC due to the negligible accuracy improvement. It takes a very long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with SGD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement a SVM model using Stochastic Gradient Descent. This should improve the relatively long training times of the previous model. We will compare the performance of the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the model\n",
    "svm_sgd_model = SGDClassifier(alpha=0.1,\n",
    "                              fit_intercept=True,\n",
    "                              l1_ratio=0.0,\n",
    "                              learning_rate='optimal',\n",
    "                              loss='hinge',\n",
    "                              n_iter_no_change=5,\n",
    "                              n_jobs=-1,\n",
    "                              penalty='l2',\n",
    "                              random_state=137)\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "svm_sgd_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', svm_sgd_model)\n",
    "])\n",
    "\n",
    "# Define the cross validation method on training holdout\n",
    "skf = StratifiedKFold(n_splits=5, random_state=137, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a pipeline using SGD with a linear Support Vector Classifier. We will again use a StandardScaler to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74205686 0.72319043 0.72956705 0.72485194 0.78402482]\n",
      "\n",
      "Overall ROC-AUC: 0.7407 (+/- 0.0226)\n"
     ]
    }
   ],
   "source": [
    "# Implement CV using the shorter method\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rocauc_svm_sgd = cross_val_score(estimator=svm_sgd_pipe,\n",
    "                                X=X_train_holdout,\n",
    "                                y=y_train_holdout,\n",
    "                                cv=skf,\n",
    "                                scoring = \"roc_auc\",\n",
    "                                n_jobs=-1)\n",
    "print(rocauc_svm_sgd)\n",
    "\n",
    "# Calculate mean and std deviation of the accuracies\n",
    "mean_ra_sgd = np.mean(rocauc_svm_sgd)\n",
    "std_ra_sgd = np.std(rocauc_svm_sgd)\n",
    "\n",
    "print(f\"\\nOverall ROC-AUC: {mean_ra_sgd:.4f} (+/- {std_ra_sgd:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stochastic Gradient Descent classifier has a ROC-AUC of approximately 74% on the cross validation set. It has a slightly higher variance between folds than the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Performance on Holdout Set: 0.7151\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "svm_sgd_pipe.fit(X_train_holdout, y_train_holdout)\n",
    "\n",
    "#prediction on the test set\n",
    "final_y_pred_prob = svm_sgd_pipe.decision_function(X_test_holdout)#[:,1] # Using decision_function instead of predict_proba\n",
    "\n",
    "#calculating the ROC-AUC performance\n",
    "final_performance = roc_auc_score(y_test_holdout, final_y_pred_prob)\n",
    "print(f\"Final Model Performance on Holdout Set: {final_performance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model using gradient descent achieves an AUC of 71.5% on the holdout set. This is roughly the same the previous SVM model with much less computation time. It is interesting to note that the difference between the validation set and cross validation scores is higher with this model. This suggests that SGD may be more prone to overfitting.\n",
    "\n",
    "However there is a trade off that happens because of this SGD does iterative steps approximating the gradient and when it finds the minima its only a local minima of the loss function. There might be many local minima that are within the loss function and the one chosen due to the step size may not be the global minima. Another thing, the Stochastic GD takes a lot more steps in its epoch time than a normal gradient descent making it more computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the previous models, we will use a grid search to tune the hyperparameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'classifier__alpha': 0.01, 'classifier__eta0': 0.01, 'classifier__learning_rate': 'invscaling'}\n",
      "Best cross-validation score:  0.7232288266892251\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter space\n",
    "param_grid = {'classifier__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "              'classifier__learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "              'classifier__eta0': [0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "# Use GridSearchCV\n",
    "grid_search = GridSearchCV(svm_sgd_pipe, param_grid, cv=skf, n_jobs=-1, scoring='roc_auc')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous models, the hyperparameters we have chosen yield similar results to the default parameters. By choosing a more narrow range of parameters, we could possibly improve the performance of the model. However, this finding increases our confidence in SKL's default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some advantages of the Logistic Regression : \n",
    "- Much easier to set up and train\n",
    "- Simple to interpret\n",
    "- In smaller dimensions it is less prone to overfit\n",
    "- Can be used for multinomial cases\n",
    "- small training time \n",
    "- can interpret the weights of the model\n",
    "\n",
    "Some advantages of the Support Vector Machine:\n",
    "- Better in datasets with higher dimensions \n",
    "- Memory efficent \n",
    "- Where other models fail having more dimensions than samples SVM prevails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Feature Importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting feature importance of a model involves quantifying the contribution of each feature to the model's predictions. This proves gives us insight into which features are the most influential to predicitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: age, Coefficient: -0.031947338639167475\n",
      "Feature: campaign, Coefficient: -0.14374625831484428\n",
      "Feature: pdays, Coefficient: 0.008555916465196382\n",
      "Feature: previous, Coefficient: 0.035648638479505716\n",
      "Feature: emp.var.rate, Coefficient: -2.296944936932174\n",
      "Feature: cons.price.idx, Coefficient: 1.170873119193251\n",
      "Feature: cons.conf.idx, Coefficient: 0.1352688587232521\n",
      "Feature: euribor3m, Coefficient: 0.39094694670362856\n",
      "Feature: nr.employed, Coefficient: 0.4294420960159024\n",
      "Feature: y, Coefficient: -0.05308982127026549\n",
      "Feature: job_blue-collar, Coefficient: -0.012972533339141293\n",
      "Feature: job_entrepreneur, Coefficient: -0.028207133020040485\n",
      "Feature: job_housemaid, Coefficient: -0.013020127899129031\n",
      "Feature: job_management, Coefficient: 0.01755126991816828\n",
      "Feature: job_retired, Coefficient: -0.028003620854794693\n",
      "Feature: job_self-employed, Coefficient: -0.03834298488707904\n",
      "Feature: job_services, Coefficient: 0.02072712168029786\n",
      "Feature: job_student, Coefficient: 0.023190440416738588\n",
      "Feature: job_technician, Coefficient: 0.002516955121919846\n",
      "Feature: job_unemployed, Coefficient: -0.035229435653224485\n",
      "Feature: job_unknown, Coefficient: 0.0105855212179452\n",
      "Feature: marital_married, Coefficient: 0.015827739118181965\n",
      "Feature: marital_single, Coefficient: 0.011714171297434921\n",
      "Feature: marital_unknown, Coefficient: 0.02027528021921067\n",
      "Feature: education_basic.6y, Coefficient: -0.005699170904422787\n",
      "Feature: education_basic.9y, Coefficient: 0.014887359882927812\n",
      "Feature: education_high.school, Coefficient: 0.01700819437385606\n",
      "Feature: education_illiterate, Coefficient: 0.006150254424650663\n",
      "Feature: education_professional.course, Coefficient: 0.06174826777295113\n",
      "Feature: education_university.degree, Coefficient: 0.002710434023506785\n",
      "Feature: education_unknown, Coefficient: -0.05168610929765809\n",
      "Feature: default_yes, Coefficient: -0.01845926554369793\n",
      "Feature: housing_yes, Coefficient: 0.003188322129817075\n",
      "Feature: loan_yes, Coefficient: -0.37068511406102855\n",
      "Feature: contact_telephone, Coefficient: 0.15527593212760282\n",
      "Feature: month_aug, Coefficient: 0.020856781245383507\n",
      "Feature: month_dec, Coefficient: 0.0233871927291922\n",
      "Feature: month_jul, Coefficient: -0.20203808462632866\n",
      "Feature: month_jun, Coefficient: 0.17441962110158432\n",
      "Feature: month_mar, Coefficient: -0.21327030681649944\n",
      "Feature: month_may, Coefficient: -0.1419207767362895\n",
      "Feature: month_nov, Coefficient: 0.011389156884111348\n",
      "Feature: month_oct, Coefficient: 0.026603522876660585\n",
      "Feature: month_sep, Coefficient: -0.07309539529820003\n",
      "Feature: day_of_week_mon, Coefficient: 0.0484117402262177\n",
      "Feature: day_of_week_thu, Coefficient: 0.02929590092131423\n",
      "Feature: day_of_week_tue, Coefficient: 0.06676384611182141\n",
      "Feature: day_of_week_wed, Coefficient: 0.18952728061870747\n",
      "Feature: poutcome_not contacted, Coefficient: 0.31935767656274994\n",
      "Feature: poutcome_success, Coefficient: -0.014150875363708645\n",
      "Feature: age_range_Middle-Age, Coefficient: 0.044343968809760125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Access the logistic regression model within the pipeline\n",
    "lr_model_in_pipeline = lr_pipe.named_steps['classifier']\n",
    "\n",
    "# Access the coefficients\n",
    "coefficients = lr_model_in_pipeline.coef_[0]  \n",
    "\n",
    "# Print the coefficients and their corresponding feature names\n",
    "for coef, feature_name in zip(coefficients, df):\n",
    "   print(f\"Feature: {feature_name}, Coefficient: {coef}\")\n",
    "\n",
    "#for coef, name in zip(weights, X_train):\n",
    "#   print(name, 'has weight of', coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the weights of the features, we can clearly see the positive and negative coefficients that have importance to the classification. The features with a positive coefficients like \"Previous\" and \"Pdays\" have a positive effect on the log-odds of the positive class. An increase in the value of these variables tends to increase the likelihood of the event we are predicting. The features with negative coefficients like \"Campaign\" and \"Emp.var.rate\" have a negative effect on the log-odds of the positive class. An increase in the value of these variables tends to decrease the likelihood of the event we are predicting. Looking at the magnitude of the coefficient can help us understand the impact. For instance, \"Emp.var.rate\" has a negative coefficient of -2.2969 which indicates there is a strong level of impact vs the other features like \"Campaign\" that also has a negative coefficient but one that is at -0.1437. While there is impact from \"Campaign\", the impact from \"Emp.var.rate\" is much more impactful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Support Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vectors are the data points that lie closest to the decision boundary (hyperplane) and play a significant role in defining the decision boundary and the margin in an SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10008, 51)\n",
      "(10008,)\n",
      "[6480 3528]\n"
     ]
    }
   ],
   "source": [
    "# look at the support vectors\n",
    "print(svm_model.support_vectors_.shape)\n",
    "print(svm_model.support_.shape)\n",
    "print(svm_model.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the SVM.Model is strange. According to support_verctors.shape, our model has 10008 support vectors, and each support vector has 51 features. This is unusual because that would indicate that each data point is a support vector with the total amount of features in our model being included (51). Such a large number of support vectors could be an indication of overfitting. Some remedies for overfitting would include reviewing the hyperparametrs, high value of the regularization parameter may lead to more support vectors. SVMs are sensitive to the scale of features, and improper scaling can affect the support vector selection. Finally, while it's not impossible to have 51 features with all data points as support vectors, it's an unusual scenario that suggests issues worth taking a look at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "\n",
    "We believe that the use of pipelines and grid search to tune the hyperparameters of the models is exceptional work. It also makes the code more readable and prevents data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "@article{Moro2014ADA,\n",
    "  title={A data-driven approach to predict the success of bank telemarketing},\n",
    "  author={S{\\'e}rgio Moro and P. Cortez and Paulo Rita},\n",
    "  journal={Decis. Support Syst.},\n",
    "  year={2014},\n",
    "  volume={62},\n",
    "  pages={22-31},\n",
    "  url={https://api.semanticscholar.org/CorpusID:14181100}\n",
    "}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
