{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Lab01: Portugese Bank Marketing Data'\n",
    "subtitle: \"MSDS 7331\"\n",
    "authors: \"Anthony Burton-Cordova, Will Jones, Nick Sager\"\n",
    "date: September 24, 2023\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For an introduction to the data, business understanding, and explanation of the the dataset, please see [Lab01](Lab01.ipynb), which contains the exploratory data analysis (EDA) from Lab 01. This notebook will focus on the modeling of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "### Reference only - delete before submitting\n",
    "\n",
    "| Category                 | Available | Requirements |\n",
    "|--------------------------|-----------|--------------|\n",
    "| Total Points             | 100       | Total             |\n",
    "| Create Models            | 50        | Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. |\n",
    "| Model Advantages         | 10        | Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail. |\n",
    "| Interpret Feature Importance | 30    | Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important? |\n",
    "| Interpret Support Vectors   | 10     | Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC modelâ€” then analyze the support vectors from the subsampled dataset. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Process Data\n",
    "\n",
    "The following code chunks are explained in more detail in [Lab01](Lab01.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Choose File\n",
    "# RawBank = \"https://raw.githubusercontent.com/NickSager/DS7331_Projects/main/data/bank-additional-full.csv\"\n",
    "RawBank = \"data/bank-additional-full.csv\"\n",
    "\n",
    "# Read the CSV file with a semicolon ; separator\n",
    "bank = pd.read_csv(RawBank, sep=';')\n",
    "\n",
    "# Get info on the dataset\n",
    "# print(bank.info())\n",
    "# bank.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# let's set those values to NaN, so that Pandas understand they are missing\n",
    "df = bank.copy() # make a copy of the dataframe\n",
    "df = df.replace(to_replace = 'unknown', value = np.nan) # replace unknown with NaN (not a number)\n",
    "df = df.replace(to_replace = 999, value = np.nan) # replace 999 with NaN (not a number)\n",
    "df = df.replace(to_replace = 'nonexistent', value = np.nan) # replace nonexistent with NaN (not a number)\n",
    "\n",
    "# print (df.info())\n",
    "# df.describe() # scroll over to see the values\n",
    "\n",
    "# From course material \"01. Pandas.ibynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change NA Categoricals to 'unknown'\n",
    "df['job'] = df['job'].fillna('unknown')\n",
    "df['marital'] = df['marital'].fillna('unknown')\n",
    "df['education'] = df['education'].fillna('unknown')\n",
    "\n",
    "# Change NA Credit history values to 'no'\n",
    "df['default'] = df['default'].fillna('no')\n",
    "df['housing'] = df['housing'].fillna('no')\n",
    "df['loan'] = df['loan'].fillna('no')\n",
    "\n",
    "# Change NA Previous Outcome to 'not contacted'\n",
    "df['poutcome'] = df['poutcome'].fillna('not contacted')\n",
    "\n",
    "# Change NA pdays to the mean\n",
    "df['pdays'] = df['pdays'].fillna(df['pdays'].mean())\n",
    "\n",
    "# Change NA Duration to '999'\n",
    "df['duration'] = df['duration'].fillna(999)\n",
    "\n",
    "# let's break up the age variable\n",
    "df['age_range'] = pd.cut(df.age,[0,40,60,1e6],3,labels=['Young','Middle-Age','Old']) # this creates a new variable\n",
    "# df.age_range.describe()\n",
    "\n",
    "# print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 53 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   age                            41188 non-null  int64  \n",
      " 1   duration                       41188 non-null  float64\n",
      " 2   campaign                       41188 non-null  int64  \n",
      " 3   pdays                          41188 non-null  float64\n",
      " 4   previous                       41188 non-null  int64  \n",
      " 5   emp.var.rate                   41188 non-null  float64\n",
      " 6   cons.price.idx                 41188 non-null  float64\n",
      " 7   cons.conf.idx                  41188 non-null  float64\n",
      " 8   euribor3m                      41188 non-null  float64\n",
      " 9   nr.employed                    41188 non-null  float64\n",
      " 10  y                              41188 non-null  object \n",
      " 11  job_blue-collar                41188 non-null  uint8  \n",
      " 12  job_entrepreneur               41188 non-null  uint8  \n",
      " 13  job_housemaid                  41188 non-null  uint8  \n",
      " 14  job_management                 41188 non-null  uint8  \n",
      " 15  job_retired                    41188 non-null  uint8  \n",
      " 16  job_self-employed              41188 non-null  uint8  \n",
      " 17  job_services                   41188 non-null  uint8  \n",
      " 18  job_student                    41188 non-null  uint8  \n",
      " 19  job_technician                 41188 non-null  uint8  \n",
      " 20  job_unemployed                 41188 non-null  uint8  \n",
      " 21  job_unknown                    41188 non-null  uint8  \n",
      " 22  marital_married                41188 non-null  uint8  \n",
      " 23  marital_single                 41188 non-null  uint8  \n",
      " 24  marital_unknown                41188 non-null  uint8  \n",
      " 25  education_basic.6y             41188 non-null  uint8  \n",
      " 26  education_basic.9y             41188 non-null  uint8  \n",
      " 27  education_high.school          41188 non-null  uint8  \n",
      " 28  education_illiterate           41188 non-null  uint8  \n",
      " 29  education_professional.course  41188 non-null  uint8  \n",
      " 30  education_university.degree    41188 non-null  uint8  \n",
      " 31  education_unknown              41188 non-null  uint8  \n",
      " 32  default_yes                    41188 non-null  uint8  \n",
      " 33  housing_yes                    41188 non-null  uint8  \n",
      " 34  loan_yes                       41188 non-null  uint8  \n",
      " 35  contact_telephone              41188 non-null  uint8  \n",
      " 36  month_aug                      41188 non-null  uint8  \n",
      " 37  month_dec                      41188 non-null  uint8  \n",
      " 38  month_jul                      41188 non-null  uint8  \n",
      " 39  month_jun                      41188 non-null  uint8  \n",
      " 40  month_mar                      41188 non-null  uint8  \n",
      " 41  month_may                      41188 non-null  uint8  \n",
      " 42  month_nov                      41188 non-null  uint8  \n",
      " 43  month_oct                      41188 non-null  uint8  \n",
      " 44  month_sep                      41188 non-null  uint8  \n",
      " 45  day_of_week_mon                41188 non-null  uint8  \n",
      " 46  day_of_week_thu                41188 non-null  uint8  \n",
      " 47  day_of_week_tue                41188 non-null  uint8  \n",
      " 48  day_of_week_wed                41188 non-null  uint8  \n",
      " 49  poutcome_not contacted         41188 non-null  uint8  \n",
      " 50  poutcome_success               41188 non-null  uint8  \n",
      " 51  age_range_Middle-Age           41188 non-null  uint8  \n",
      " 52  age_range_Old                  41188 non-null  uint8  \n",
      "dtypes: float64(7), int64(3), object(1), uint8(42)\n",
      "memory usage: 5.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Convert all features to numeric using dummy variables\n",
    "df = pd.get_dummies(df, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome', 'age_range'], drop_first=True)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models\n",
    "\n",
    "In this section, we will create Logistic Regression and Support Vector Machine (SVM) models to classify whether a customer will subscribe to a term deposit. We will split the data into training and validation sets using an 80/20 split. ROC - AUC will be used to evaluate the models.\n",
    "\n",
    "Due to previous work in Lab01.ipynb it was shown that our response variable, whether a customer will subscribe to a term deposit, was highly unbalanced with only 11% of the variable data being 'yes' so for the metric we will choose ROC-AUC. The common metric accuracy would not be good for this model due to it only being able to assess the correct number of true predictions in the samples from the dataset but fails to take into account the false positives. \n",
    "\n",
    "The Area Under the Curve is great for classification models and understands the relationship between the true positive rate and the false positive rate. It's value is contained in the range of 0.5 and 1 being that if the models auc is 0.5 it predicts no better than a flip of a coin at random or 1 being that it is the perfect classifier. \n",
    "\n",
    "The code in this section is adopted from the course material in the notebook '04. Logits and SVM.ipynb'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will change the data into a format that Scikit-Learn can use. We will also split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Consider deleting duration for practicality\n",
    "if 'duration' in df: del df['duration']\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "X = df.drop(columns=['y']).values\n",
    "y = df['y'].values\n",
    "    \n",
    "# Split into training and test sets\n",
    "X_train_holdout, X_test_holdout, y_train_holdout, y_test_holdout = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=137, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'holdout' splits will be used to evaluate the final model on unseen data. We will additionally use cross-validation to tune the hyperparameters of the models.\n",
    "\n",
    "Next we will define the parts of the pipeline that will be used to transform the data and fit the models. We will use a StandardScaler to scale the data, and experiment with using PCA to reduce the dimensionality of the data. We will use a LogisticRegression model and a SVC model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the model\n",
    "lr_model = LogisticRegression(penalty='l2',\n",
    "                           C=1.0,\n",
    "                           class_weight=None,\n",
    "                           solver='liblinear',\n",
    "                           random_state=137,\n",
    "                           max_iter=150)\n",
    "\n",
    "# Define the pipeline\n",
    "lr_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', lr_model)\n",
    "])\n",
    "\n",
    "# Define the cross validation method on training holdout\n",
    "skf = StratifiedKFold(n_splits=5, random_state=137, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC for fold: 1 is 0.8019\n",
      "ROC-AUC for fold: 2 is 0.7869\n",
      "ROC-AUC for fold: 3 is 0.7796\n",
      "ROC-AUC for fold: 4 is 0.7929\n",
      "ROC-AUC for fold: 5 is 0.8078\n",
      "\n",
      "Overall ROC-AUC: 0.7938 (+/- 0.0101)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "rocauc_lr = []\n",
    "\n",
    "for train_index, val_index in skf.split(X_train_holdout, y_train_holdout):\n",
    "    # Splitting the data\n",
    "    X_train, X_val = X_train_holdout[train_index], X_train_holdout[val_index]\n",
    "    y_train, y_val = y_train_holdout[train_index], y_train_holdout[val_index]\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    lr_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    # y_pred = lr_pipe.predict(X_val) # Incorrect\n",
    "    y_pred_prob = lr_pipe.predict_proba(X_val)[:, 1] # Need to use probability estimates here\n",
    "\n",
    "    # Calculate ROC-AUC\n",
    "    ra = roc_auc_score(y_val, y_pred_prob)\n",
    "    rocauc_lr.append(ra)\n",
    "    \n",
    "    # Optionally, print the ROC-AUC for each fold\n",
    "    print(f\"ROC-AUC for fold: {len(rocauc_lr)} is {ra:.4f}\")\n",
    "\n",
    "# Calculate mean and std deviation of the ROC-AUC's\n",
    "mean_ra_lr = np.mean(rocauc_lr)\n",
    "std_ra_lr = np.std(rocauc_lr)\n",
    "\n",
    "print(f\"\\nOverall ROC-AUC: {mean_ra_lr:.4f} (+/- {std_ra_lr:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8018649  0.78693328 0.77961894 0.79288743 0.80776593]\n",
      "\n",
      "Overall ROC-AUC: 0.7938 (+/- 0.0101)\n"
     ]
    }
   ],
   "source": [
    "# Implement the same thing a different way for practice\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rocauc_lr2 = cross_val_score(estimator=lr_pipe,\n",
    "                                X=X_train_holdout,\n",
    "                                y=y_train_holdout,\n",
    "                                cv=skf,\n",
    "                                scoring=\"roc_auc\",\n",
    "                                n_jobs=-1)\n",
    "print(rocauc_lr2)\n",
    "\n",
    "# Calculate mean and std deviation of the accuracies\n",
    "mean_ra_lr2 = np.mean(rocauc_lr2)\n",
    "std_ra_lr2 = np.std(rocauc_lr2)\n",
    "\n",
    "print(f\"\\nOverall ROC-AUC: {mean_ra_lr2:.4f} (+/- {std_ra_lr2:.4f})\")\n",
    "# Same results and faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC of each fold is approximately 79%. This is about 14% lower than when 'duration' is included. By removing it, we have a more practical model with only a small loss in predictive power. This small loss in predictive power is a good trade off to have a model that is useful in the real world.\n",
    "\n",
    "Next, we will evaluate the final model on the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Performance on Holdout Set: 0.7785\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "lr_pipe.fit(X_train_holdout, y_train_holdout)\n",
    "\n",
    "#prediction on the test set\n",
    "final_y_pred_prob = lr_pipe.predict_proba(X_test_holdout)[:,1    ]\n",
    "\n",
    "#calculating the ROC-AUC performance\n",
    "final_performance = roc_auc_score(y_test_holdout, final_y_pred_prob)\n",
    "print(f\"Final Model Performance on Holdout Set: {final_performance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model achieves an accuracy of 77.85% on the holdout set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, we will use a grid search with the cross validation object to tune the hyperparameters of the Logistic Regression model. The parameters in the grid search are a rough estimate, and further tuning could be done on a narrower range once the best parameters are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "240 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.76378575        nan 0.76378215 0.78807711\n",
      " 0.78807596 0.78841665 0.78807649 0.78807605        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.78866927\n",
      "        nan 0.78864609 0.78924224 0.78924191 0.78927001 0.78923292\n",
      " 0.78914497        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.79115461        nan 0.79013289 0.79078378\n",
      " 0.79078024 0.79077576 0.79032413 0.7897229         nan        nan\n",
      "        nan        nan        nan        nan        nan 0.79069629\n",
      "        nan 0.78985292 0.79066169 0.79065933 0.79065998 0.7904622\n",
      " 0.78981274        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.79059675        nan 0.78982191 0.79059744\n",
      " 0.79060075 0.79059662 0.79047438 0.78982161        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.79059237\n",
      "        nan 0.78982338 0.79058574 0.79058712 0.79058547 0.79047668\n",
      " 0.78982146        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'classifier__C': 0.1, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\n",
      "Best cross-validation score (ROC-AUC):  0.7911546135814282\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "param_grid_logistic = {\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(lr_pipe, param_grid_logistic,\n",
    "                           cv=skf, n_jobs=-1, error_score=np.nan,\n",
    "                           scoring='roc_auc')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score (ROC-AUC): \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters are C=0.1 and penalty='l1' using the 'liblinear' solver. The best accuracy ROC-AUC 79.1%, which is comparable to the baseline model without tuning. Many combinations of parameters wither failed to converge or failed due to mismatch of the solver and penalty, but the grid search can be forced to run by setting `error_score=np.nan`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement our Support Vector machine which will be done in similar fashion as the Logistic Regression in the section above. One model will be made using a pipeline and the other with the cross_val_score function. We will also implement the svm later on using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the model\n",
    "svm_model = SVC(C=1.0,\n",
    "            kernel='rbf',\n",
    "            gamma='scale',\n",
    "            class_weight=None,\n",
    "            random_state=137)\n",
    "\n",
    "# Define the pipeline\n",
    "svm_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', svm_model)\n",
    "])\n",
    "\n",
    "# Define the cross validation method on training holdout\n",
    "skf = StratifiedKFold(n_splits=5, random_state=137, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a pipeline for the SVM model. We will use a StandardScaler to scale the data, and the 'rbf' kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71681981 0.72287402 0.71873191 0.71550502 0.72138281]\n",
      "\n",
      "Overall ROC-AUC: 0.7191 (+/- 0.0027)\n"
     ]
    }
   ],
   "source": [
    "# Implement CV using the shorter method with much faster processing speed\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rocauc_svm = cross_val_score(estimator=svm_pipe,\n",
    "                                X=X_train_holdout,\n",
    "                                y=y_train_holdout,\n",
    "                                scoring = \"roc_auc\",\n",
    "                                cv=skf,\n",
    "                                n_jobs=-1)\n",
    "print(rocauc_svm)\n",
    "\n",
    "# Calculate mean and std deviation of the accuracies\n",
    "mean_ra_svm = np.mean(rocauc_svm)\n",
    "std_ra_svm = np.std(rocauc_svm)\n",
    "\n",
    "print(f\"\\nOverall ROC-AUC: {mean_ra_svm:.4f} (+/- {std_ra_svm:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC of each fold is approximately 71%. This is about 8% lower than the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Performance on Holdout Set: 0.7030\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "svm_pipe.fit(X_train_holdout, y_train_holdout)\n",
    "\n",
    "#prediction on the test set\n",
    "final_y_pred_prob = svm_pipe.decision_function(X_test_holdout)#[:,1] # Using decision_function instead of predict_proba\n",
    "\n",
    "#calculating the ROC-AUC performance\n",
    "final_performance = roc_auc_score(y_test_holdout, final_y_pred_prob)\n",
    "print(f\"Final Model Performance on Holdout Set: {final_performance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model achieves an AUC of 70.3% on the holdout set. This is approximately 8% lower than the Logistic Regression model as predicted by the cross validation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, we will use a grid search with the cross validation object to tune the hyperparameters of the SVM model. The parameters in the grid search are a rough estimate, and further tuning could be done on a narrower range once the best parameters are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this. 51m to run on my machine\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define hyperparameter space\n",
    "# param_grid = {'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "#               'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# # Use GridSearchCV\n",
    "# grid_search = GridSearchCV(svm_pipe, param_grid, cv=skf, n_jobs=-1, scoring='roc_auc')\n",
    "# grid_search.fit(X, y)\n",
    "\n",
    "# print(\"Best parameters: \", grid_search.best_params_)\n",
    "# print(\"Best cross-validation score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search yields an optimal `C` of `1.0`, and `gamma` of `0.01`. The accuracy of the model with these parameters is 89.9%, which is equivalent to the model using the default parameters. The grid search was not evaluated using ROC-AUC due to the negligible accuracy improvement. It takes a very long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with SGD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement a SVM model using Stochastic Gradient Descent. This should improve the relatively long training times of the previous model. We will compare the performance of the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the model\n",
    "svm_sgd_model = SGDClassifier(alpha=0.1,\n",
    "                              fit_intercept=True,\n",
    "                              l1_ratio=0.0,\n",
    "                              learning_rate='optimal',\n",
    "                              loss='hinge',\n",
    "                              n_iter_no_change=5,\n",
    "                              n_jobs=-1,\n",
    "                              penalty='l2',\n",
    "                              random_state=137)\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "svm_sgd_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', svm_sgd_model)\n",
    "])\n",
    "\n",
    "# Define the cross validation method on training holdout\n",
    "skf = StratifiedKFold(n_splits=5, random_state=137, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a pipeline using SGD with a linear Support Vector Classifier. We will again use a StandardScaler to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74205686 0.72319043 0.72956705 0.72485194 0.78402482]\n",
      "\n",
      "Overall ROC-AUC: 0.7407 (+/- 0.0226)\n"
     ]
    }
   ],
   "source": [
    "# Implement CV using the shorter method\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rocauc_svm_sgd = cross_val_score(estimator=svm_sgd_pipe,\n",
    "                                X=X_train_holdout,\n",
    "                                y=y_train_holdout,\n",
    "                                cv=skf,\n",
    "                                scoring = \"roc_auc\",\n",
    "                                n_jobs=-1)\n",
    "print(rocauc_svm_sgd)\n",
    "\n",
    "# Calculate mean and std deviation of the accuracies\n",
    "mean_ra_sgd = np.mean(rocauc_svm_sgd)\n",
    "std_ra_sgd = np.std(rocauc_svm_sgd)\n",
    "\n",
    "print(f\"\\nOverall ROC-AUC: {mean_ra_sgd:.4f} (+/- {std_ra_sgd:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stochastic Gradient Descent classifier has a ROC-AUC of approximately 74% on the cross validation set. It has a slightly higher variance between folds than the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Performance on Holdout Set: 0.7151\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "svm_sgd_pipe.fit(X_train_holdout, y_train_holdout)\n",
    "\n",
    "#prediction on the test set\n",
    "final_y_pred_prob = svm_sgd_pipe.decision_function(X_test_holdout)#[:,1] # Using decision_function instead of predict_proba\n",
    "\n",
    "#calculating the ROC-AUC performance\n",
    "final_performance = roc_auc_score(y_test_holdout, final_y_pred_prob)\n",
    "print(f\"Final Model Performance on Holdout Set: {final_performance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model using gradient descent achieves an AUC of 71.5% on the holdout set. This is roughly the same the previous SVM model with much less computation time. It is interesting to note that the difference between the validation set and cross validation scores is higher with this model. This suggests that SGD may be more prone to overfitting.\n",
    "\n",
    "However there is a trade off that happens because of this SGD does iterative steps approximating the gradient and when it finds the minima its only a local minima of the loss function. There might be many local minima that are within the loss function and the one chosen due to the step size may not be the global minima. Another thing, the Stochastic GD takes a lot more steps in its epoch time than a normal gradient descent making it more computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the previous models, we will use a grid search to tune the hyperparameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'classifier__alpha': 0.01, 'classifier__eta0': 0.01, 'classifier__learning_rate': 'invscaling'}\n",
      "Best cross-validation score:  0.7232288266892251\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter space\n",
    "param_grid = {'classifier__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "              'classifier__learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "              'classifier__eta0': [0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "# Use GridSearchCV\n",
    "grid_search = GridSearchCV(svm_sgd_pipe, param_grid, cv=skf, n_jobs=-1, scoring='roc_auc')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous models, the hyperparameters we have chosen yield similar results to the default parameters. By choosing a more narrow range of parameters, we could possibly improve the performance of the model. However, this finding increases our confidence in SKL's default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some advantages of the Logistic Regression : \n",
    "- Much easier to set up and train\n",
    "- Simple to interpret\n",
    "- In smaller dimensions it is less prone to overfit\n",
    "- Can be used for multinomial cases\n",
    "- small training time \n",
    "- can interpret the weights of the model\n",
    "\n",
    "Some advantages of the Support Vector Machine:\n",
    "- Better in datasets with higher dimensions \n",
    "- Memory efficent \n",
    "- Where other models fail having more dimensions than samples SVM prevails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Feature Importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Support Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "\n",
    "We believe that the use of pipelines and grid search to tune the hyperparameters of the models is exceptional work. It also makes the code more readable and prevents data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "@article{Moro2014ADA,\n",
    "  title={A data-driven approach to predict the success of bank telemarketing},\n",
    "  author={S{\\'e}rgio Moro and P. Cortez and Paulo Rita},\n",
    "  journal={Decis. Support Syst.},\n",
    "  year={2014},\n",
    "  volume={62},\n",
    "  pages={22-31},\n",
    "  url={https://api.semanticscholar.org/CorpusID:14181100}\n",
    "}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
