---
title: "QTW Case Study 6"
author: "Nicholas Sager"
date: last-modified
format: 
  html: 
    toc: true
    toc-location: left
    toc-depth: 4
    embed-resources: true
    # self-contained-math: true
    code-fold: true
filters: [appendix.lua]
jupyter: python3
execute:
  cache: true
  cache-directory: .cache
---

```{python setup}
# | output: false
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
import os

from IPython.display import Markdown

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import (
    KFold,
)
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    roc_auc_score,
    roc_curve,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
)
```
Don't ensemble
must use CV
Use advanced methods
EDA, missing data, different thresholds
custom function for monetary loss?
XGB, NN, SVM? - all with CV, only a few minutes
$17 false positive (1 class), $5 false negative
calculate loss per sample - whole dataset
fairly high accuracy - above 90%

## Introduction
The goal of this Case Study is to predict the class of a variable that we don't know anything about. The data is anonymized and we are not concerned with explaining the predictions. False positives of class '1' incur a cost of $100. False positives of class '0' incur a cost of $25. Given these, our goal is to develop a model that minimizes financial loss.

## Data Processing and Exploration
```{python}
directory = "./Case-Study-7-final/data" if os.path.exists("./Case-Study-7-final/data") else "./data"
data = pd.read_csv(f"{directory}/final_project(5).csv")

print(data)
print(data.dtypes)
```
The data contains 160,000 records containing 50 features and the target variable, a binary label of '1' or '0'. Most features are numeric, but three features are text. These include what appear to be continent, month, day of the week, a percentage change, and a price change stored as a string. We will convert the percentage change and price change to numeric values to be included in our model.

### Summary Statistics

Our dataset has two classes. They are labelled '1.0' and '0.0' and are split relatively evenly. There are approximately 95,000 records labelled '0' and 64,000 records labelled '1'.

```{python}
# | label: fig-target-dist
# | fig-cap: "Distribution of Target Variable (y)."
# print(data["y"].value_counts())

sns.set_theme(style="whitegrid")

# Plot the distribution of target variable (category)
# plt.figure(figsize=(8, 6))
sns.countplot(x="y", data=data, palette="viridis")
plt.title("Distribution of Target Variable")
plt.xlabel("Record Classification")
plt.ylabel("Count")
plt.show()
```

We will explore some of the features to gain intuition on the dataset. Starting with the categorical variables:

The 'x24' variable appears to represent continents. Asia is the most common continent in the dataset with approximately 140,000 records. The other continents have fewer records, with America having the fewest at around 5,000 records and Europe splitting the difference.

```{python}
# | label: fig-x24-dist
# | fig-cap: "Distribution of Continent Variable. Asia is the most common."
# print(data["x24"].value_counts())

sorted_data = data['x24'].value_counts().index

# plt.figure(figsize=(8, 6))
sns.countplot(x="x24", data=data, order=sorted_data, palette="viridis")
plt.title("Count Plot of x24 (Continent)")
plt.xlabel("Category")
plt.ylabel("Count")
plt.show()
```

The 'x29' variable appears to represent months. The majority of records are for summer months, with July and June having the most records. December has the fewest records. Many of the month labels in the dataset are irregular or misspelled so these are mapped to the full month names for clarity.

```{python}
month_mapping = {
    'July': 'July',
    'Jun': 'June',
    'Aug': 'August',
    'May': 'May',
    'sept.': 'September',
    'Apr': 'April',
    'Oct': 'October',
    'Mar': 'March',
    'Nov': 'November',
    'Feb': 'February',
    'Dev': 'December',
    'January': 'January'
}

data['x29'] = data['x29'].replace(month_mapping)
```
```{python}
# | label: fig-x29-dist
# | fig-cap: "Distribution of Month Variable. The majority of records are for summer months."
# print(data["x29"].value_counts())
month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']

# plt.figure(figsize=(8, 6))
sns.countplot(x="x29", data=data, order=month_order, palette="viridis")
plt.title("Count Plot of x29 (Month)")
plt.xlabel("Category")
plt.ylabel("Count")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()
```

The 'x30' variable appears to represent the day of the week. The majority of records are in the middle of the work week with Tuesday, Wednesday, and Thursday having the most records. Monday and Friday have fewer records. Again, the spellings are irregular so these are mapped to the full day names for clarity.

```{python}
day_mapping = {
  "monday": "Monday",
  "tuesday": "Tuesday",
  "wednesday": "Wednesday",
  "thurday": "Thursday",
  "friday": "Friday",
}

data['x30'] = data['x30'].replace(day_mapping)
```
```{python}
# | label: fig-x30-dist
# | fig-cap: "Distribution of Day Variable. Most records are in the middle of the work week."
# print(data["x30"].value_counts())
day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']

# plt.figure(figsize=(8, 6))
sns.countplot(x="x30", data=data, order=day_order, palette="viridis")
plt.title("Count Plot of x30 (Day)")
plt.xlabel("Category")
plt.ylabel("Count")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()
```

The 'x32' variable appears to represent a percentage change. The distribution of this variable is shown below. Most records tend toward smaller absolute values of changes, with the most common changes being 0.0%, and 0.01%. We have chosen to keep this feature as a categorical variable for now, but we could consider converting it to a numeric variable in the future. There are relatively few categories and they are nicely rounded so it's doubtful that the model would benefit from the increased complexity of a continuous variable for this feature. The 0.0% category was originally counted as separate from -0.0%. We have combined them for simplicity since they are the same.

```{python}
data['x32'] = data['x32'].replace('-0.0%', '0.0%')
# print(data['x32'].value_counts())
```

```{python}
# | label: fig-x32-dist
# | fig-cap: "Distribution of Percentage Change Variable. Most records tend toward smaller changes."
# print(data["x32"].value_counts())
change_order = ['-0.05%', '-0.04%', '-0.03%', '-0.02%', '-0.01%', '0.0%', '0.01%', '0.02%', '0.03%', '0.04%', '0.05%']

# plt.figure(figsize=(8, 6))
sns.countplot(x="x32", data=data, order=change_order, palette="viridis")
plt.title("Count Plot of x32 (Percentage Change)")
plt.xlabel("Category")
plt.ylabel("Count")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()
```

'x37' value represents monetary values. The distribution is shown below of the values. The distribution centers around $0 with a normal distribution ranging from $-4000 to $4000. We have choosen to convert this to a float value.

```{python}
data['x37'] = data['x37'].replace('[\$,]', '', regex=True).astype(float)
```
```{python}
# | label: fig-x37-dist
# | fig-cap: "Distribution of Monetary variable. Records show a normal distribution ranging from -4000 to 4000.
#Histogtam
plt.figure(figsize=(10, 6))
sns.histplot(data['x37'], bins=30, kde=True)
plt.title('Distribution of x37')
plt.xlabel('x37')
plt.ylabel('Frequency')
```

Here we are checking for missing values within the data set.
```(python}
missing_by_column = data.isnull().sum()
total_missing = missing_by_column.sum()

missing_summary = pd.DataFrame(missing_by_column, columns=["Missing Values"])
missing_summary.loc["Total"] = total_missing

print(missing_summary)
```

```(python}
#insert missing data handling here
```

For the model to handle the data, preprocessing needs to occur to make the data into a injestable format. The mutli-object variables are one hot encoded to be used for the model.

```(python}
day_to_num = {
    'Monday': 1,
    'Tuesday': 2,
    'Wednesday': 3,
    'Thursday': 4,
    'Friday': 5,
    'Saturday': 6,
    'Sunday': 7
}
# Convert the column
data['x30'] = data['x30'].map(day_to_num)

month_to_num = {
    'July': 6,
    'June' : 5,
    'August' : 7,
    'May' : 4,
    'September' : 8,
    'April' : 3,
    'October' : 9,
    'March' : 2,
    'November' : 10,
    'February' : 1,
    'December' : 11,
    'January' : 0
}
# Convert the column
data['x29'] = data['x29'].map(month_to_num)

country_to_num = {
    'america' : 0, 
    'eurpoe' : 1, 
    'asia' : 2
}
#Convert the column
data['x24'] = data['x24'].map(country_to_num)
```
# TODO: Continue work here
To better understand the distribution of the other features, we can plot a histogram of the 'f4' variable. The 'f4' variable appears to be also uniformly distributed. We could also plot histograms of other features, but for brevity, we will only show the distribution of 'f4'.

```{python}
# | label: fig-feature-dist
# | fig-cap: "Distribution of F4 Variable. This variable appears to be Uniformly distributed."

# plt.figure(figsize=(8, 6))
# sns.histplot(data["mass"], bins=5, kde=False, edgecolor="black")
plt.hist(data['f4'], bins=5, edgecolor='black')
plt.title("Histogram of 'f4' Variable")
plt.xlabel("f4")
plt.ylabel("Frequency")
plt.show()
```

### Missing Data

As shown in the table below, this dataset is complete and doesn't have any missing values.

```{python}
# | output: false
# | eval: true
missing_by_column = data.isnull().sum()
total_missing = missing_by_column.sum()

missing_summary = pd.DataFrame(missing_by_column, columns=["Missing Values"])
missing_summary.loc["Total"] = total_missing

print(missing_summary)
```

```{python}
# | label: tbl-missing-data
# | tbl-cap: "Missing data summary by attribute. There is no missing data in this dataset."

missing_md = missing_summary.to_markdown()
Markdown(missing_md)
```

## Model Building
To classify the records, we will fit a dense Neural Network. We will evaluate the models on validation data using accuracy and other classification metrics such as precision and recall.

### Dense Neural Network
```{python}
scaler = StandardScaler()
y = data["label"]
X = data.drop(["label"], axis=1)
X = scaler.fit_transform(X)
```

The data is first transformed to a suitable format for the classifier. The features are scaled using the StandardScaler. We then split the data into five folds for cross-validation and compute the out-of-fold predictions. The model is trained with early stopping and dropout layers to prevent over-fitting. The out-of-fold predictions are stored for the entire dataset and used to evaluate overall performance.

The Neural Network model has three hidden layers with 64, 32, and 16 units and a final output layer with a sigmoid activation function. The activation functions for the hidden layers are RELU, and Sigmoid activation is chosen for the final layer since we are doing a binary classification. The model is compiled with the Adam optimizer and binary cross-entropy loss function. The model is trained for 20 epochs with a batch size of 64. Early stopping is used to stop training when the validation loss does not improve for three consecutive epochs. Dropout layers are used with 20% dropout rate between each hidden layer to prevent over-fitting. More complicated models were explored, but quickly reach diminishing returns in performance, so the simpler model was chosen.

```{python NN with KFold Cross-Validation}
# | output: false
# | eval: true

def build_model(input_shape):
    model = tf.keras.models.Sequential([
        # tf.keras.layers.Dense(256, activation='relu', input_shape=(input_shape,)),
        # tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(10, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
    return model


kf = KFold(n_splits=5, shuffle=True, random_state=42)
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss", patience=3, restore_best_weights=True
)

oof_preds = np.zeros(len(X))
oof_true = np.zeros(len(X))

# Lists to store the metrics for each fold
accuracy_list = []
precision_list = []
recall_list = []
f1_list = []

train_losses = []
val_losses = []

for train_index, val_index in kf.split(X):
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    model = build_model(X_train.shape[1])
    history = model.fit(
        X_train,
        y_train,
        epochs=10,
        batch_size=64,
        verbose=1,
        validation_data=(X_val, y_val),
        callbacks=[early_stopping],
    )

    # Append losses for each epoch
    train_losses.append(history.history['loss'])
    val_losses.append(history.history['val_loss'])

    y_pred = model.predict(X_val).flatten()

    # Store the predictions and true values
    oof_preds[val_index] = y_pred
    oof_true[val_index] = y_val

    # Convert probabilities to binary predictions
    y_pred_binary = (y_pred > 0.5).astype(int)

    # Calculate and store metrics for the current fold
    accuracy_list.append(accuracy_score(y_val, y_pred_binary))
    precision_list.append(precision_score(y_val, y_pred_binary))
    recall_list.append(recall_score(y_val, y_pred_binary))
    f1_list.append(f1_score(y_val, y_pred_binary))


# Convert from probabilities to binary predictions
oof_preds_binary = (oof_preds > 0.5).astype(int)

# Compute metrics for the entire dataset
accuracy = accuracy_score(oof_true, oof_preds_binary)
precision = precision_score(oof_true, oof_preds_binary)
recall = recall_score(oof_true, oof_preds_binary)
f1 = f1_score(oof_true, oof_preds_binary)

# Calculate mean and standard deviation of metrics for each fold
mean_accuracy = np.mean(accuracy_list)
std_accuracy = np.std(accuracy_list)
mean_precision = np.mean(precision_list)
std_precision = np.std(precision_list)
mean_recall = np.mean(recall_list)
std_recall = np.std(recall_list)
mean_f1 = np.mean(f1_list)
std_f1 = np.std(f1_list)
```

```{python}
# | output: false
# | eval: false

# | label: fig-train-val-loss
# | fig-cap: "Training and Validation Losses per fold for each epoch in the Neural Network model."

# Plotting training and validation losses
fig, ax = plt.subplots()

colors_train = plt.cm.Blues(np.linspace(0.3, 0.7, len(train_losses)))
colors_val = plt.cm.Oranges(np.linspace(0.3, 0.7, len(val_losses)))

for i in range(len(train_losses)):
    ax.plot(train_losses[i], label=f'Train Fold {i+1}', color=colors_train[i], linestyle='-', marker='o')
    ax.plot(val_losses[i], label=f'Val Fold {i+1}', color=colors_val[i], linestyle='--', marker='x')

ax.set_title('Training and Validation Loss Per Epoch')
ax.set_xlabel('Epoch')
ax.set_ylabel('Loss')
ax.legend()
plt.show()
```

```{python}
# | output: true
# | eval: true

# | label: print-metrics-nn
print(f'Overall Accuracy: {accuracy}')
print(f'Overall Precision: {precision}')
print(f'Overall Recall: {recall}')
print(f'Overall F1 Score: {f1}')

print(f'Mean Accuracy: {mean_accuracy}, Std Accuracy: {std_accuracy}')
print(f'Mean Precision: {mean_precision}, Std Precision: {std_precision}')
print(f'Mean Recall: {mean_recall}, Std Recall: {std_recall}')
print(f'Mean F1 Score: {mean_f1}, Std F1 Score: {std_f1}')

# Optional print model info
print(model.summary())
```


```{python}
# | output: false
# | eval: false

# | label: tbl-class-report-nn
# | tbl-cap: "Dense Neural Network Cross-Validation Classification Report"

class_report_dict = classification_report(oof_true, oof_preds_binary, output_dict=True)
class_report_df = pd.DataFrame(class_report_dict).transpose()
class_report_df.loc["accuracy"] = class_report_df.loc["accuracy"].drop(
    ["precision", "recall"]
)
class_report_df = class_report_df.rename(
    index={"0": "background", "1": "Signal"}
)
class_report_df = class_report_df.round(3)
class_report_df = class_report_df.fillna("")

metrics_md = class_report_df.to_markdown()
Markdown(metrics_md)
```

## Results and Discussion
### Classifier Performance
This classifier is largely effective at distinguishing between Signal and Background. The Neural Network is equally successful at identifying both classes, which is expected given the roughly equal class distribution. The mean accuracy of the model is approximately 88%. For records that are signal, the model has a precision of 0.85 and a recall of 0.90 This means that 85% of records identified as signal are actually signal, while 90% of to total signal are identified. For background records, 90% are identified correctly and 84% percent are identified. The ROC-AUC score is also fairly high at 0.95 and roughly represents the probability of a correct prediction. The classifier has a low false positive rate and high true positive rate across many thresholds. The confusion matrix, ROC plot, and final classification report are shown below.

```{python}
conf_matrix = confusion_matrix(y, oof_preds_binary)
class_report = classification_report(y, oof_preds_binary)
roc_auc = roc_auc_score(y, oof_preds)
```

```{python}
# | label: fig-conf-matrix
# | fig-cap: "Dense NN Classifier Confusion Matrix. Out-of-fold predictions are used to evaluate the model."

# Plot confusion matrix with Seaborn
# plt.figure(figsize=(8, 6))
sns.heatmap(
    conf_matrix,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=["Background", "Signal"],
    yticklabels=["Background", "Signal"],
)
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()
```
```{python}
# | label: fig-roc-curve
# | fig-cap: "Dense NN Classifier ROC Curve. Out-of-fold predictions are used to evaluate the model."

# Plot ROC curve
fpr, tpr, _ = roc_curve(y, oof_preds)
# plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color="blue", lw=2, label=f"ROC curve (area = {roc_auc:.4f})")
plt.plot([0, 1], [0, 1], color="gray", lw=2, linestyle="--")
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.show()
```

```{python}
# Display the classification report
print("\nClassification Report:")
print(class_report)
print(f"\nROC-AUC Score: {roc_auc:.4f}")
```

```{python}
# | label: tbl-class-report
# | tbl-cap: "Dense NN Classification Report. The table shows the precision, recall, f1-score, and support for each class. Out-of-fold predictions are used to evaluate the model."

class_report_dict = classification_report(y, oof_preds_binary, output_dict=True)
class_report_df = pd.DataFrame(class_report_dict).transpose()
class_report_df.loc["accuracy"] = class_report_df.loc["accuracy"].drop(
    ["precision", "recall"]
)
class_report_df = class_report_df.rename(
    index={"0": "Background", "1": "Signal"}
)
class_report_df = class_report_df.round(3)
class_report_df = class_report_df.fillna("")

metrics_md = class_report_df.to_markdown()
Markdown(metrics_md)
```







