---
title: "QTW Case Study 6"
author: "Nicholas Sager"
date: last-modified
format: 
  html: 
    toc: true
    toc-location: left
    toc-depth: 4
    embed-resources: true
    # self-contained-math: true
    code-fold: true
filters: [appendix.lua]
jupyter: python3
execute:
  cache: true
  cache-directory: .cache
---

```{python setup}
# | output: false
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
import os

from IPython.display import Markdown
import xgboost as xgb
import tensorflow as tf

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import (
    KFold,
)
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    roc_auc_score,
    roc_curve,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    make_scorer,
)
```
Don't ensemble
must use CV
Use advanced methods
EDA, missing data, different thresholds
custom function for monetary loss?
XGB, NN, SVM? - all with CV, only a few minutes
$17 false positive (1 class), $5 false negative
calculate loss per sample - whole dataset
fairly high accuracy - above 90%

## Introduction
The goal of this Case Study is to predict the class of a variable that we don't know anything about. The data is anonymized and we are not concerned with explaining the predictions. False positives of class '1' incur a cost of $17 False positives of class '0' incur a cost of $5. Given these, our goal is to develop a model that minimizes financial loss.

## Data Processing and Exploration
```{python}
directory = "./Case-Study-7-final/data" if os.path.exists("./Case-Study-7-final/data") else "./data"
data = pd.read_csv(f"{directory}/final_project(5).csv")

print(data)
print(data.dtypes)
```
The data contains 160,000 records containing 50 features and the target variable, a binary label of '1' or '0'. Most features are numeric, but three features are text. These include what appear to be continent, month, day of the week, a percentage change, and a price change stored as a string. We will convert the percentage change and price change to numeric values to be included in our model.

### Summary Statistics

Our dataset has two classes. They are labelled '1.0' and '0.0' and are split relatively evenly. There are approximately 95,000 records labelled '0' and 64,000 records labelled '1'.

```{python}
# | label: fig-target-dist
# | fig-cap: "Distribution of Target Variable (y)."
# print(data["y"].value_counts())

sns.set_theme(style="whitegrid")

# Plot the distribution of target variable (category)
# plt.figure(figsize=(8, 6))
sns.countplot(x="y", data=data, palette="viridis")
plt.title("Distribution of Target Variable")
plt.xlabel("Record Classification")
plt.ylabel("Count")
plt.show()
```

We will explore some of the features to gain intuition on the dataset. Starting with the categorical variables:

The 'x24' variable appears to represent continents. Asia is the most common continent in the dataset with approximately 140,000 records. The other continents have fewer records, with America having the fewest at around 5,000 records and Europe splitting the difference.

```{python}
# | label: fig-x24-dist
# | fig-cap: "Distribution of Continent Variable. Asia is the most common."
# print(data["x24"].value_counts())

sorted_data = data['x24'].value_counts().index

# plt.figure(figsize=(8, 6))
sns.countplot(x="x24", data=data, order=sorted_data, palette="viridis")
plt.title("Count Plot of x24 (Continent)")
plt.xlabel("Category")
plt.ylabel("Count")
plt.show()
```

The 'x29' variable appears to represent months. The majority of records are for summer months, with July and June having the most records. December has the fewest records. Many of the month labels in the dataset are irregular or misspelled so these are mapped to the full month names for clarity.

```{python}
month_mapping = {
    'July': 'July',
    'Jun': 'June',
    'Aug': 'August',
    'May': 'May',
    'sept.': 'September',
    'Apr': 'April',
    'Oct': 'October',
    'Mar': 'March',
    'Nov': 'November',
    'Feb': 'February',
    'Dev': 'December',
    'January': 'January'
}

data['x29'] = data['x29'].replace(month_mapping)
```
```{python}
# | label: fig-x29-dist
# | fig-cap: "Distribution of Month Variable. The majority of records are for summer months."
# print(data["x29"].value_counts())
month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']

# plt.figure(figsize=(8, 6))
sns.countplot(x="x29", data=data, order=month_order, palette="viridis")
plt.title("Count Plot of x29 (Month)")
plt.xlabel("Category")
plt.ylabel("Count")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()
```

The 'x30' variable appears to represent the day of the week. The majority of records are in the middle of the work week with Tuesday, Wednesday, and Thursday having the most records. Monday and Friday have fewer records. Again, the spellings are irregular so these are mapped to the full day names for clarity.

```{python}
day_mapping = {
  "monday": "Monday",
  "tuesday": "Tuesday",
  "wednesday": "Wednesday",
  "thurday": "Thursday",
  "friday": "Friday",
}

data['x30'] = data['x30'].replace(day_mapping)
```
```{python}
# | label: fig-x30-dist
# | fig-cap: "Distribution of Day Variable. Most records are in the middle of the work week."
# print(data["x30"].value_counts())
day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']

# plt.figure(figsize=(8, 6))
sns.countplot(x="x30", data=data, order=day_order, palette="viridis")
plt.title("Count Plot of x30 (Day)")
plt.xlabel("Category")
plt.ylabel("Count")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()
```

The 'x32' variable appears to represent a percentage change. The distribution of this variable is shown below. Most records tend toward smaller absolute values of changes, with the most common changes being 0.0%, and 0.01%. We have chosen to keep this feature as a categorical variable for now, but we could consider converting it to a numeric variable in the future. There are relatively few categories and they are nicely rounded so it's doubtful that the model would benefit from the increased complexity of a continuous variable for this feature. The 0.0% category was originally counted as separate from -0.0%. We have combined them for simplicity since they are the same.

```{python}
data['x32'] = data['x32'].replace('-0.0%', '0.0%')
# print(data['x32'].value_counts())
```

```{python}
# | label: fig-x32-dist
# | fig-cap: "Distribution of Percentage Change Variable. Most records tend toward smaller changes."
# print(data["x32"].value_counts())
change_order = ['-0.05%', '-0.04%', '-0.03%', '-0.02%', '-0.01%', '0.0%', '0.01%', '0.02%', '0.03%', '0.04%', '0.05%']

# plt.figure(figsize=(8, 6))
sns.countplot(x="x32", data=data, order=change_order, palette="viridis")
plt.title("Count Plot of x32 (Percentage Change)")
plt.xlabel("Category")
plt.ylabel("Count")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()
```

'x37' value represents monetary values. The distribution is shown below of the values. The distribution centers around $0 with a normal distribution ranging from $-4000 to $4000. We have choosen to convert this to a float value.

```{python}
data['x37'] = data['x37'].replace('[\$,]', '', regex=True).astype(float)
```
```{python}
# | label: fig-x37-dist
# | fig-cap: "Distribution of Monetary variable. Records show a normal distribution ranging from -4000 to 4000.
#Histogtam
plt.figure(figsize=(10, 6))
sns.histplot(data['x37'], bins=30, kde=True)
plt.title('Distribution of x37')
plt.xlabel('x37')
plt.ylabel('Frequency')
plt.show()
```

To better understand the distribution of the other features, we can plot a histogram of the 'x20' variable. The 'x20' variable appears to be normally distributed. We could also plot histograms of other features, but for brevity, we will only show the distribution of 'x20'.

```{python}
# | label: fig-feature-dist
# | fig-cap: "Distribution of x20 Variable. This variable appears to be Normally distributed."

# plt.figure(figsize=(8, 6))
# sns.histplot(data["mass"], bins=5, kde=False, edgecolor="black")
plt.hist(data['x20'], bins=20, edgecolor='black')
plt.title("Histogram of 'x20' Variable")
plt.xlabel("x20")
plt.ylabel("Frequency")
plt.show()
```

### Missing Data

```{python}
missing_by_column = data.isnull().sum()
total_missing = missing_by_column.sum()

missing_summary = pd.DataFrame(missing_by_column, columns=["Missing Values"])
missing_summary.loc["Total"] = total_missing

print(missing_summary)
```

Before modelling, we investigate the missing values by feature and class. The table below shows the percentage of missing values for each class. The total percentage of missing values is also shown. Values are missing nearly identically between the two target classes, and seem to be evenly distributed across the features. We conclude that the data is missing completely at random.
```{python}
missing_by_column = data.isnull().sum()
total_entries = len(data)
missing_percentage = (missing_by_column / total_entries) * 100

class0_missing = data[data["y"] == 0].isnull().sum()
class1_missing = data[data["y"] == 1].isnull().sum()
class0_percentage = (class0_missing / len(data[data["y"] == 0])) * 100
class1_percentage = (class1_missing / len(data[data["y"] == 1])) * 100

total_missing = missing_by_column.sum()
total_percentage = (total_missing / (total_entries * len(data.columns))) * 100

missing_summary = pd.DataFrame(
    {
        "Missing Values": missing_by_column,
        "Percentage": missing_percentage,
        "Percentage Class 0": class0_percentage,
        "Percentage Class 1": class1_percentage,
    }
)
# missing_summary.loc["Total"] = [total_missing, total_percentage]
missing_summary.loc["Total"] = [
    total_missing,
    total_percentage,
    (class0_missing.sum() / (len(data.columns) * len(data[data["y"] == 0])) * 100),
    (class1_missing.sum() / (len(data.columns) * len(data[data["y"] == 1])) * 100),
]

# Sort and display the top 15 missing values
sorted_missing_summary = missing_summary.drop("Total").sort_values(
    "Percentage", ascending=False
).head(10)
sorted_missing_summary = pd.concat(
    [sorted_missing_summary, missing_summary.loc[["Total"]]]
)
sorted_missing_summary = sorted_missing_summary.round(2)

print(sorted_missing_summary)
```

```{python}
# | label: tbl-missing-data
# | tbl-cap: "Missing data summary by attribute (top 10 only). The table shows the number and percentage of missing values in the dataset."

missing_md = sorted_missing_summary.to_markdown()
Markdown(missing_md)
```

The proportion of missing data is relatively low, with most features having less than 2% or 3% missing values. Because the data is missing completely at random, we can impute the missing values with the median of the feature.

```{python}
numeric_cols = data.select_dtypes(include=['number']).columns
categorical_cols = data.select_dtypes(include=['object', 'category']).columns

for col in numeric_cols:
    median_value = data[col].median()
    data[col] = data[col].fillna(median_value)

for col in categorical_cols:
    mode_value = data[col].mode()[0]
    data[col] = data[col].fillna(mode_value)

print(f"Remaining missing data: {data.isnull().sum().sum()}")
```

## Model Building
To classify the records, we will fit a dense Neural Network. We will evaluate the models on validation data using accuracy and other classification metrics such as precision and recall.

### Data Preparation
Before fitting models, we need to prepare the data. We will scale the features using the StandardScaler. Categorical variables will be one-hot encoded. We will then split the data into five folds for cross-validation. We will fit the model on four folds and validate on the remaining fold. We will store the out-of-fold predictions and use them to evaluate the model.
```{python}
# Make arrays
target_column = 'y'
X_raw = data.drop(columns=[target_column])
y = data[target_column]

# Categorical to onehot and numerical to standard scaler
numeric_cols = X_raw.select_dtypes(include=['number']).columns
categorical_cols = X_raw.select_dtypes(include=['object', 'category']).columns

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ]
)

pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor)
])

X = pipeline.fit_transform(X_raw)

kf = KFold(n_splits=5, shuffle=True, random_state=137)
```

### Loss Functions
This task has very specific requirements. We need to minimize the financial loss incurred by incorrect predictions. False positives of class '1' incur a cost of $17. False negatives incur a penalty of $5. Since all of our machine learning frameworks allow us to do do, we will define a custom loss function to minimize the financial loss and train for that rather than the typically maximization of accuracy. The loss function will take the true labels and predicted probabilities as input and return the financial loss. We will create these for scikit-learn, XGBoost and TensorFlow.
```{python}
# Note: These don't work for tf. probably better to min normal loss and then rune threshold
# Scikit-learn
def financial_loss_skl(y_true, y_pred):
    fp = ((y_true == 0) & (y_pred == 1)).sum() * 17  # False Positives
    fn = ((y_true == 1) & (y_pred == 0)).sum() * 5   # False Negatives
    return fp + fn

financial_loss_scorer_skl = make_scorer(financial_loss_skl, greater_is_better=False)

# TensorFlow - reworked to be differentiable
def financial_loss_tf(y_true, y_pred):
    # y_pred are the predicted probabilities
    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)
    # Cost matrix
    fp_cost = 17.0
    fn_cost = 5.0
    # Calculating the cost
    loss = y_true * (1 - y_pred) * fn_cost + (1 - y_true) * y_pred * fp_cost
    return tf.reduce_mean(loss)

# XGBoost
def custom_objective_xgb(preds, dtrain):
    labels = dtrain.get_label()
    preds = 1.0 / (1.0 + np.exp(-preds))  # Transform predictions to probability
    fp_grad = 17 * (labels - preds) * (labels == 0)
    fn_grad = 5 * (preds - labels) * (labels == 1)
    grad = fp_grad + fn_grad
    hess = preds * (1 - preds)
    return grad, hess
```

### Dense Neural Network

The data is split into five folds for cross-validation and to compute the out-of-fold predictions. The model is trained with early stopping and dropout layers to prevent over-fitting. The out-of-fold predictions are stored for the entire dataset and used to evaluate overall performance.

The Neural Network model has three hidden layers with 256, 64, 32, and 16 units and a final output layer with a sigmoid activation function. The activation functions for the hidden layers are RELU, and Sigmoid activation is chosen for the final layer since we are doing a binary classification. The model is compiled with the Adam optimizer and binary cross-entropy loss function. The model is trained for 100 epochs with a batch size of 256. Early stopping is used to stop training when the validation loss does not improve for five consecutive epochs. Dropout layers are used with 20% dropout rate between each hidden layer to prevent over-fitting. More complicated models were explored, but quickly reach diminishing returns in performance, so the simpler model was chosen. Training and validation losses for each fold are shown below.

```{python NN with KFold Cross-Validation}
# | output: false
# | eval: true

# Attempt to use more of computer
tf.config.threading.set_intra_op_parallelism_threads(8)  # Set to the number of cores in your machine
tf.config.threading.set_inter_op_parallelism_threads(2)

def build_model(input_shape):
    model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(256, activation='relu', input_shape=(input_shape,)),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(16, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
    return model


kf = KFold(n_splits=5, shuffle=True, random_state=42)

oof_preds = np.zeros(len(X))
oof_true = np.zeros(len(X))

# Lists to store the metrics for each fold
accuracy_list = []
precision_list = []
recall_list = []
f1_list = []

train_losses = []
val_losses = []

for train_index, val_index in kf.split(X):
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor="val_loss", patience=5, restore_best_weights=True
    )
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    model = build_model(X_train.shape[1])
    history = model.fit(
        X_train,
        y_train,
        epochs=100,
        batch_size=256,
        verbose=1,
        validation_data=(X_val, y_val),
        callbacks=[early_stopping],
    )

    # Append losses for each epoch
    train_losses.append(history.history['loss'])
    val_losses.append(history.history['val_loss'])

    y_pred = model.predict(X_val).flatten()

    # Store the predictions and true values
    oof_preds[val_index] = y_pred
    oof_true[val_index] = y_val

    # Convert probabilities to binary predictions
    y_pred_binary = (y_pred > 0.5).astype(int)

    # Calculate and store metrics for the current fold
    accuracy_list.append(accuracy_score(y_val, y_pred_binary))
    precision_list.append(precision_score(y_val, y_pred_binary))
    recall_list.append(recall_score(y_val, y_pred_binary))
    f1_list.append(f1_score(y_val, y_pred_binary))
```

```{python}
# | output: false
# | eval: false

# | label: fig-train-val-loss
# | fig-cap: "Training and Validation Losses per fold for each epoch in the Neural Network model."

# Plotting training and validation losses
fig, ax = plt.subplots()

colors_train = plt.cm.Blues(np.linspace(0.3, 0.7, len(train_losses)))
colors_val = plt.cm.Oranges(np.linspace(0.3, 0.7, len(val_losses)))

for i in range(len(train_losses)):
    ax.plot(train_losses[i], label=f'Train Fold {i+1}', color=colors_train[i], linestyle='-', marker='o')
    ax.plot(val_losses[i], label=f'Val Fold {i+1}', color=colors_val[i], linestyle='--', marker='x')

ax.set_title('Training and Validation Loss Per Epoch')
ax.set_xlabel('Epoch')
ax.set_ylabel('Loss')
ax.legend()
plt.show()
```

The model has been trained with binary cross-entropy which effectively will maximize prediction accuracy. Since we are more concerned with financial loss, we then choose the prediction threshold that minimizes the financial loss. We calculate the financial loss across different thresholds using the financial loss function. The optimal threshold is approximately 0.8. We then evaluate the model using the optimal threshold and calculate the financial loss of approximately $0.24 per record with the Neural Network. The financial loss and its components across a range of thresholds is shown below.
```{python}
def calculate_financial_components(y_true, y_probs, threshold):
    y_pred = (y_probs >= threshold).astype(int)
    fp = np.sum((y_true == 0) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    total_loss = fp * 17 + fn * 5
    return total_loss, fp * 17, fn * 5

# Evaluate financial impact across thresholds
thresholds = np.linspace(0, 1, 101)
total_losses = []
fp_losses = []
fn_losses = []

for t in thresholds:
    total_loss, fp_loss, fn_loss = calculate_financial_components(oof_true, oof_preds, t)
    total_losses.append(total_loss)
    fp_losses.append(fp_loss)
    fn_losses.append(fn_loss)

optimal_threshold = thresholds[np.argmin(total_losses)]
print(f"Optimal threshold: {optimal_threshold}")

optimal_loss = total_losses[np.argmin(total_losses)]
avg_loss = optimal_loss / len(oof_true)
print(f"Financial loss at optimal threshold: {avg_loss}")
```

```{python}
# | output: false
# | eval: false

# | label: fig-financial-loss-threshold
# | fig-cap: "Total, False Positive, and False Negative Financial Losses Across Different Thresholds."
# plt.figure(figsize=(10, 6))
plt.plot(thresholds, total_losses, label='Total Financial Loss', color='blue')
plt.plot(thresholds, fp_losses, label='False Positives Loss', linestyle='--', color='red')
plt.plot(thresholds, fn_losses, label='False Negatives Loss', linestyle='--', color='green')

plt.title('Financial Loss Across Different Thresholds')
plt.xlabel('Threshold')
plt.ylabel('Financial Loss')
plt.legend()
plt.grid(True)
plt.show()
```

```{python}
# | output: true
# | eval: true

# | label: print-metrics-nn

# Convert from probabilities to binary predictions
oof_preds_binary = (oof_preds > optimal_threshold).astype(int)

# Compute metrics for the entire dataset
accuracy = accuracy_score(oof_true, oof_preds_binary)
precision = precision_score(oof_true, oof_preds_binary)
recall = recall_score(oof_true, oof_preds_binary)
f1 = f1_score(oof_true, oof_preds_binary)
roc_auc = roc_auc_score(oof_true, oof_preds)

# Calculate mean and standard deviation of metrics for each fold
mean_accuracy = np.mean(accuracy_list)
std_accuracy = np.std(accuracy_list)
mean_precision = np.mean(precision_list)
std_precision = np.std(precision_list)
mean_recall = np.mean(recall_list)
std_recall = np.std(recall_list)
mean_f1 = np.mean(f1_list)
std_f1 = np.std(f1_list)

print(f'Overall Accuracy: {accuracy}')
print(f'Overall Precision: {precision}')
print(f'Overall Recall: {recall}')
print(f'Overall F1 Score: {f1}')

print(f'Mean Accuracy: {mean_accuracy}, Std Accuracy: {std_accuracy}')
print(f'Mean Precision: {mean_precision}, Std Precision: {std_precision}')
print(f'Mean Recall: {mean_recall}, Std Recall: {std_recall}')
print(f'Mean F1 Score: {mean_f1}, Std F1 Score: {std_f1}')

# Optional print model info
print(model.summary())
```

In addition to the financial loss, the traditional classification metrics are calculated and displayed below in order to better understand the model's performance.
```{python}
# | output: false
# | eval: false

# | label: tbl-class-report-nn
# | tbl-cap: "Dense Neural Network Cross-Validation Classification Report"

class_report_dict = classification_report(oof_true, oof_preds_binary, output_dict=True)
class_report_df = pd.DataFrame(class_report_dict).transpose()
class_report_df.loc["accuracy"] = class_report_df.loc["accuracy"].drop(
    ["precision", "recall"]
)
class_report_df = class_report_df.rename(
    index={"0": "background", "1": "Signal"}
)
class_report_df = class_report_df.round(3)
class_report_df = class_report_df.fillna("")

metrics_md = class_report_df.to_markdown()
Markdown(metrics_md)
```

# TODO: Continue work here
### Other model and hyperparameter tuning for comparison (XGBoost?)

## Results and Discussion
### Classifier Performance
The final classifier is largely effective at reducing financial loss. As shown by the higher precision (0.95) and lower recall (0.78) for class 1 in the confusion matrix, the model is most incentivized to minimize false positives. This comes at the cost of a slight reduction in total accuracy compared to the training. Adjusting the threshold to accomplish this results in a lower precision (0.87) and higher recall (0.97) for class 0. The resulting financial loss computed on out-of sample predictions over the entire dataset is $0.24 per record or approximately $40,000. Though we don't know much else about this problem, this is presumably offset by some sort of gain for correct predictions. As shown in discussion of threshold in the neural network section above, the financial loss is relatively constant across most thresholds. Thus most of the gains have come from training the model to have accurate predictions. Tuning the threshold with these particular losses mostly serves to optimize as much as possible.

The Neural Network outperformed the XGBoost model at reducing financial loss. The table below compares the performance of the two across a variety of metrics. The confusion matrix, ROC plot, and final classification report for the final classifier are also shown below.

```{python}
# | label: tbl-model-comparison
# | tbl-cap: "Comparison between the two models. The table shows financial loss, accuracy, precision, recall, and ROC-AUC score for each model."

# Placeholder values for XGB - replace these with actual computed values
xgb_oof_preds = np.random.rand(len(oof_true))
xgb_oof_true = oof_true

# Calculate metrics for XGB
xgb_accuracy = accuracy_score(xgb_oof_true, (xgb_oof_preds >= optimal_threshold).astype(int))
xgb_precision = precision_score(xgb_oof_true, (xgb_oof_preds >= optimal_threshold).astype(int))
xgb_recall = recall_score(xgb_oof_true, (xgb_oof_preds >= optimal_threshold).astype(int))
xgb_roc_auc = roc_auc_score(xgb_oof_true, xgb_oof_preds)

#TODO: Change these out for calculated ones or rework outside this block
xgb_total_financial_loss = calculate_financial_components(xgb_oof_true, xgb_oof_preds, optimal_threshold)[0]
xgb_avg_financial_loss = xgb_total_financial_loss / len(xgb_oof_true)

comparison = {
    "Model": ["Neural Network", "XGBoost"],
    "Total Financial Loss": [optimal_loss, xgb_total_financial_loss],
    "Average Financial Loss Per Sample": [avg_loss, xgb_avg_financial_loss],
    "Overall Accuracy": [mean_accuracy, xgb_accuracy],
    "Precision": [mean_precision, xgb_precision],
    "Recall": [mean_recall, xgb_recall],
    "ROC-AUC": [roc_auc, xgb_roc_auc]
}

comp_df = pd.DataFrame(comparison)
class_report_df = class_report_df.round(3)
class_report_df = class_report_df.fillna("")

comp_md = comp_df.to_markdown()
Markdown(comp_md)
```

```{python}
conf_matrix = confusion_matrix(y, oof_preds_binary)
class_report = classification_report(y, oof_preds_binary)
roc_auc = roc_auc_score(y, oof_preds)
```

```{python}
# | label: fig-conf-matrix
# | fig-cap: "Dense NN Classifier Confusion Matrix. Out-of-fold predictions are used to evaluate the model."

# Plot confusion matrix with Seaborn
# plt.figure(figsize=(8, 6))
sns.heatmap(
    conf_matrix,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=["Class 0", "Class 1"],
    yticklabels=["Class 0", "Class 1"],
)
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()
```
```{python}
# | label: fig-roc-curve
# | fig-cap: "Dense NN Classifier ROC Curve. Out-of-fold predictions are used to evaluate the model."

# Plot ROC curve
fpr, tpr, _ = roc_curve(y, oof_preds)
# plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color="blue", lw=2, label=f"ROC curve (area = {roc_auc:.4f})")
plt.plot([0, 1], [0, 1], color="gray", lw=2, linestyle="--")
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.show()
```

```{python}
# Display the classification report
print("\nClassification Report:")
print(class_report)
print(f"\nROC-AUC Score: {roc_auc:.4f}")
```

```{python}
# | label: tbl-class-report
# | tbl-cap: "Dense NN Classification Report. The table shows the precision, recall, f1-score, and support for each class. Out-of-fold predictions are used to evaluate the model."

class_report_dict = classification_report(y, oof_preds_binary, output_dict=True)
class_report_df = pd.DataFrame(class_report_dict).transpose()
class_report_df.loc["accuracy"] = class_report_df.loc["accuracy"].drop(
    ["precision", "recall"]
)
class_report_df = class_report_df.rename(
    index={"0": "Clas 0", "1": "Class 1"}
)
class_report_df = class_report_df.round(3)
class_report_df = class_report_df.fillna("")

metrics_md = class_report_df.to_markdown()
Markdown(metrics_md)
```







